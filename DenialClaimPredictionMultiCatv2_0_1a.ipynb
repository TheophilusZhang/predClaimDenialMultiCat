{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "N8T7J6ooJ1Ac"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\ayxbhg\\.venvs\\py313\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import pyodbc\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import (\n",
        "    average_precision_score, roc_auc_score, precision_recall_curve,\n",
        "    f1_score, precision_score, recall_score, confusion_matrix, classification_report,\n",
        "    accuracy_score\n",
        "    )\n",
        "from sklearn.preprocessing import label_binarize\n",
        "import time\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold, RandomizedSearchCV\n",
        "from sklearn.metrics import precision_recall_curve, average_precision_score, auc\n",
        "from xgboost import XGBClassifier\n",
        "import shap\n",
        "import os\n",
        "from datetime import datetime\n",
        "from joblib import dump, load\n",
        "import warnings\n",
        "from sklearn.exceptions import FitFailedWarning\n",
        "import xgboost as xgb\n",
        "import optuna"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R-894FADJ-TZ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Connection successful!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\ayxbhg\\AppData\\Local\\Temp\\ipykernel_20900\\656024245.py:25: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
            "  chunk_generator = pd.read_sql(sql_query, cnxn, chunksize=chunksize)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded chunk 1 with 100000 rows\n",
            "Loaded chunk 2 with 100000 rows\n",
            "Loaded chunk 3 with 100000 rows\n",
            "Loaded chunk 4 with 100000 rows\n",
            "Loaded chunk 5 with 100000 rows\n",
            "Loaded chunk 6 with 100000 rows\n",
            "Loaded chunk 7 with 100000 rows\n",
            "Loaded chunk 8 with 100000 rows\n",
            "Loaded chunk 9 with 100000 rows\n",
            "Loaded chunk 10 with 100000 rows\n",
            "Loaded chunk 11 with 100000 rows\n",
            "Loaded chunk 12 with 100000 rows\n",
            "Loaded chunk 13 with 100000 rows\n",
            "Loaded chunk 14 with 100000 rows\n",
            "Loaded chunk 15 with 100000 rows\n",
            "Loaded chunk 16 with 100000 rows\n",
            "Loaded chunk 17 with 100000 rows\n",
            "Loaded chunk 18 with 100000 rows\n",
            "Loaded chunk 19 with 100000 rows\n",
            "Loaded chunk 20 with 100000 rows\n",
            "Loaded chunk 21 with 100000 rows\n",
            "Loaded chunk 22 with 100000 rows\n",
            "Loaded chunk 23 with 100000 rows\n",
            "Loaded chunk 24 with 100000 rows\n",
            "Loaded chunk 25 with 65920 rows\n",
            "\n",
            " Total rows combined: 2465920\n",
            "\n",
            "Total rows returned: 2465920\n",
            "\n",
            "Top 5 rows of the DataFrame:\n",
            "  Clinic  TPCLIID  LIATPCLIid   ServiceDt                     Service  \\\n",
            "0    AHK    11443       11443  2025-04-06  Methadone Maintenance Week   \n",
            "1    AHK    11444       11444  2025-04-13  Methadone Maintenance Week   \n",
            "2    AHK    11445       11445  2025-04-20  Methadone Maintenance Week   \n",
            "3    AHK    11446       11446  2025-04-27  Methadone Maintenance Week   \n",
            "4    AHK    11447       11447  2025-05-04  Methadone Maintenance Week   \n",
            "\n",
            "     ClaimID  AmountCharged CPTCode  ClientID ClaimBillDate  ... TotalAdj  \\\n",
            "0  143009513         254.93   H0020        46    2025-08-27  ...      0.0   \n",
            "1  143009514         254.93   H0020        46    2025-08-27  ...      0.0   \n",
            "2  143009515         254.93   H0020        46    2025-08-27  ...      0.0   \n",
            "3  143009516         254.93   H0020        46    2025-08-27  ...      0.0   \n",
            "4  143009517         254.93   H0020        46    2025-08-27  ...      0.0   \n",
            "\n",
            "  TotalVoid CoPay Deduc  CoIns CltResp Balance MultiFlag  SameDayCli  \\\n",
            "0       0.0   0.0   0.0    0.0     0.0     0.0         N           0   \n",
            "1       0.0   0.0   0.0    0.0     0.0     0.0         N           0   \n",
            "2       0.0   0.0   0.0    0.0     0.0     0.0         N           0   \n",
            "3       0.0   0.0   0.0    0.0     0.0     0.0         N           0   \n",
            "4       0.0   0.0   0.0    0.0     0.0     0.0         N           0   \n",
            "\n",
            "   DaysBetServiceToBilling  \n",
            "0                      143  \n",
            "1                      136  \n",
            "2                      129  \n",
            "3                      122  \n",
            "4                      115  \n",
            "\n",
            "[5 rows x 29 columns]\n"
          ]
        }
      ],
      "source": [
        "def read_from_azure_in_chunks(chunksize=100000):\n",
        "    try:\n",
        "        # Set up connection\n",
        "        cnxn = pyodbc.connect(\n",
        "            'Driver={ODBC Driver 17 for SQL Server};'\n",
        "            'Server=XXXXXXXXXXXX;'\n",
        "            'Authentication=ActiveDirectoryPassword;'\n",
        "            'Database=XXXXXXXXXXXX;'\n",
        "            'UID=xxxxxxxxx;'\n",
        "            'PWD=xxxxxxxxx;'\n",
        "        )\n",
        "        print(\"Connection successful!\")\n",
        "\n",
        "        # Define SQL query\n",
        "        sql_query = \"\"\"\n",
        "        SELECT\n",
        "            *\n",
        "        FROM\n",
        "            pats.vw_ClaimsDenialPrediction\n",
        "        WHERE\n",
        "            LIATPCLIid IS NOT NULL            \n",
        "        \"\"\"\n",
        "\n",
        "        # Load data in chunks and concatenate\n",
        "        chunk_generator = pd.read_sql(sql_query, cnxn, chunksize=chunksize)\n",
        "        chunks = []\n",
        "        for i, chunk in enumerate(chunk_generator):\n",
        "            print(f\"Loaded chunk {i+1} with {len(chunk)} rows\")\n",
        "            chunks.append(chunk)\n",
        "\n",
        "        full_df = pd.concat(chunks, ignore_index=True)\n",
        "        print(f\"\\n Total rows combined: {len(full_df)}\")\n",
        "        return full_df\n",
        "\n",
        "    except pyodbc.DatabaseError as e:\n",
        "        print('Database Error:', str(e))\n",
        "    except pyodbc.Error as e:\n",
        "        print('Connection Error:', str(e))\n",
        "    finally:\n",
        "        try:\n",
        "            cnxn.close()\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    df = read_from_azure_in_chunks()\n",
        "\n",
        "    if df is not None:\n",
        "        print(f\"\\nTotal rows returned: {len(df)}\\n\")\n",
        "        print(\"Top 5 rows of the DataFrame:\")\n",
        "        print(df.head())\n",
        "    else:\n",
        "        print(\"No data returned.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Total rows before deduplication: 2465920\n",
            "No duplication detected.\n"
          ]
        }
      ],
      "source": [
        "print(f\"\\nTotal rows before deduplication: {len(df)}\")\n",
        "\n",
        "duplicate_count = df.duplicated().sum()\n",
        "if duplicate_count == 0:\n",
        "    print(\"No duplication detected.\")\n",
        "else:\n",
        "    print(f\"Number of duplicated rows: {duplicate_count}\")\n",
        "    df = df.drop_duplicates()\n",
        "    print(f\"Total rows after deduplication: {len(df)}\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Clinic                           0\n",
            "TPCLIID                          0\n",
            "LIATPCLIid                       0\n",
            "ServiceDt                        0\n",
            "Service                          0\n",
            "ClaimID                          0\n",
            "AmountCharged                    0\n",
            "CPTCode                          0\n",
            "ClientID                         0\n",
            "ClaimBillDate                    0\n",
            "Payer                            0\n",
            "Provider                         0\n",
            "AuthStatus                       0\n",
            "eligStatus                       0\n",
            "DenialFlag                       0\n",
            "lastActDt                        0\n",
            "cliANSI1                   1587687\n",
            "cliANSI2                   2351312\n",
            "TotalPaid                        0\n",
            "TotalAdj                         0\n",
            "TotalVoid                        0\n",
            "CoPay                            0\n",
            "Deduc                            0\n",
            "CoIns                            0\n",
            "CltResp                          0\n",
            "Balance                          0\n",
            "MultiFlag                        0\n",
            "SameDayCli                       0\n",
            "DaysBetServiceToBilling          0\n",
            "dtype: int64\n"
          ]
        }
      ],
      "source": [
        "print(df.isnull().sum()) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Columns dropped: ['cliANSI1', 'cliANSI2', 'TotalPaid', 'TotalAdj', 'TotalVoid', 'DenialFlag', 'CoPay', 'Deduc', 'CoIns', 'CltResp', 'ClientResp.', 'Balance', 'tpcliAge']\n",
            "Rows dropped due to NULLs : 0\n",
            "Total rows after cleaning: 2465920\n",
            "Sampled training rows: 2000000\n",
            "Unseen (sampled) rows: 10000\n"
          ]
        }
      ],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Full pipeline (native categoricals, random UNSEEN sampling, Optuna tuning, SHAP):\n",
        "- Keep `ClientID` (do NOT drop it).\n",
        "- Use XGBoost **native categoricals** (no one-hot) with `enable_categorical=True`.\n",
        "- Build **UNSEEN** by random sampling from the remainder after taking the training sample (no date cutoff).\n",
        "- Engineer compact **date features** and drop raw datetime columns before modeling.\n",
        "- Optuna tunes on CV PR-AUC (wider search for more trial diversity); choose threshold from OOF.\n",
        "- Compare **F1/precision/recall** at each trial's OOF-chosen threshold on TEST and UNSEEN.\n",
        "- Save artifacts; score UNSEEN and Azure data; export **SHAP Top-K** explanations.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# ======================================\n",
        "# CLEANUP + PREP\n",
        "# ======================================\n",
        "\n",
        "# Drop unnecessary columns\n",
        "cols_to_drop = [\n",
        "    'cliANSI1', 'cliANSI2', 'TotalPaid', 'TotalAdj', 'TotalVoid','DenialFlag',\n",
        "    'CoPay', 'Deduc', 'CoIns', 'CltResp', 'ClientResp.', 'Balance', 'tpcliAge'\n",
        "]\n",
        "df.drop(columns=cols_to_drop, inplace=True, errors='ignore')\n",
        "print(f\"Columns dropped: {cols_to_drop}\")\n",
        "\n",
        "# Drop rows with nulls in important columns\n",
        "before_dropna = len(df)\n",
        "df = df.dropna(subset=['ServiceDt', 'Service', 'CPTCode'])\n",
        "after_dropna = len(df)\n",
        "print(f\"Rows dropped due to NULLs : {before_dropna - after_dropna}\")\n",
        "print(f\"Total rows after cleaning: {len(df)}\")\n",
        "\n",
        "# Step 1: Drop ID columns (KEEP ClientID)\n",
        "df = df.drop(columns=['TPCLIID', 'ClaimID', 'LIATPCLIid'], errors='ignore')\n",
        "\n",
        "# Step 2: Target type\n",
        "df['MultiFlag'] = df['MultiFlag'].astype('category')\n",
        "\n",
        "\n",
        "# Ensure datetime dtype for feature engineering\n",
        "for c in ['ClaimBillDate', 'ServiceDt', 'lastActDt']:\n",
        "    df[c] = pd.to_datetime(df[c], errors='coerce')\n",
        "\n",
        "# --------------------------------------\n",
        "# Build TRAIN sample and random UNSEEN\n",
        "# --------------------------------------\n",
        "\n",
        "# Use full df as pool\n",
        "df_train_pool = df.copy()\n",
        "\n",
        "# --- Randomly sample up to 1,000,000 rows from training pool ---\n",
        "n_sample = min(2_000_000, len(df_train_pool))\n",
        "df_sample = df_train_pool.sample(n=n_sample, random_state=123)\n",
        "print(\"Sampled training rows:\", df_sample.shape[0])\n",
        "\n",
        "# --- UNSEEN: sample from remainder (no date logic) ---\n",
        "remainder = df_train_pool.drop(index=df_sample.index)\n",
        "n_unseen = min(10_000, len(remainder))\n",
        "df_unseen = remainder.sample(n=n_unseen, random_state=123).copy()\n",
        "print(\"Unseen (sampled) rows:\", df_unseen.shape[0])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NaN/blank counts in MultiFlag by ClaimBillDate (overall):\n",
            "Empty DataFrame\n",
            "Columns: [ClaimDate, NaN_MultiFlag_Count]\n",
            "Index: []\n",
            "MultiFlag missing/blank: 0 / 2465920 (0.00%)\n",
            "MultiFlag distribution (valid labels only):\n",
            "MultiFlag\n",
            "N    1587649\n",
            "Z     594457\n",
            "P     243209\n",
            "F      40605\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# (Optional) QC: MultiFlag completeness/validity by ClaimBillDate on the *original* df before dates are dropped\n",
        "try:\n",
        "    if 'MultiFlag' not in df.columns:\n",
        "        raise KeyError(\"MultiFlag not in df; nothing to QC.\")\n",
        "\n",
        "    # Safe, non-mutating date parsing\n",
        "    claim_dates = pd.to_datetime(df['ClaimBillDate'], errors='coerce').dt.date\n",
        "\n",
        "    # Treat NaN or blank strings as missing\n",
        "    mf_obj = df['MultiFlag'].astype('object')\n",
        "    blank_mask = mf_obj.isna() | (mf_obj.astype(str).str.strip() == '')\n",
        "\n",
        "    nan_counts = (\n",
        "        pd.DataFrame({'ClaimDate': claim_dates, 'is_blank': blank_mask})\n",
        "          .loc[lambda d: d['is_blank']]\n",
        "          .groupby('ClaimDate')\n",
        "          .size()\n",
        "          .reset_index(name='NaN_MultiFlag_Count')\n",
        "          .sort_values('NaN_MultiFlag_Count', ascending=False)\n",
        "    )\n",
        "\n",
        "    print(\"NaN/blank counts in MultiFlag by ClaimBillDate (overall):\")\n",
        "    print(nan_counts.head(20))\n",
        "\n",
        "    # Overall missing rate\n",
        "    total = len(df)\n",
        "    missing = int(blank_mask.sum())\n",
        "    print(f\"MultiFlag missing/blank: {missing} / {total} ({missing/total:.2%})\")\n",
        "\n",
        "    # Valid/invalid label check (expected from your case: N, Z, P, F)\n",
        "    expected = {'N', 'Z', 'P', 'F'}\n",
        "    valid_mask = ~blank_mask & mf_obj.isin(expected)\n",
        "    invalid_mask = ~blank_mask & ~mf_obj.isin(expected)\n",
        "\n",
        "    print(\"MultiFlag distribution (valid labels only):\")\n",
        "    print(df.loc[valid_mask, 'MultiFlag'].value_counts())\n",
        "\n",
        "    if invalid_mask.any():\n",
        "        print(\"⚠️ Found invalid MultiFlag labels (showing top 10):\")\n",
        "        print(df.loc[invalid_mask, 'MultiFlag'].value_counts().head(10))\n",
        "\n",
        "except Exception as _e:\n",
        "    print(\"[Info] Skipped MultiFlag QC:\", _e)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Drop raw datetime columns (no date features) ---\n",
        "drop_dt = ['ServiceDt', 'ClaimBillDate', 'lastActDt']\n",
        "df_sample = df_sample.drop(columns=drop_dt, errors='ignore')\n",
        "df_unseen = df_unseen.drop(columns=drop_dt, errors='ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train full labels: {'N': 772630, 'Z': 289384, 'P': 118193, 'F': 19793}\n",
            "Val full labels: {'N': 257543, 'Z': 96462, 'P': 39398, 'F': 6597}\n",
            "Test full labels: {'N': 257543, 'Z': 96461, 'P': 39398, 'F': 6598}\n",
            "Stage A class weights: {0: 0.7765683444857177, 1: 1.4039356997449517}\n",
            "Stage B class weights: {'Z': 0.492275546217713, 'P': 1.2052885252651737, 'F': 7.197325653850688}\n"
          ]
        }
      ],
      "source": [
        "# ======================================\n",
        "# Hierarchical XGBoost:  Stage A (N vs Denied)  →  Stage B (Z/P/F)\n",
        "# F1 floors: Stage A = 0.90, Stage B = 0.80\n",
        "# ======================================\n",
        "from pandas.api.types import CategoricalDtype as _CatDType\n",
        "\n",
        "# (Optional) for Azure scoring at the end\n",
        "try:\n",
        "    import pyodbc\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "# --- Preconditions\n",
        "assert 'df_sample' in globals(), \"df_sample must exist\"\n",
        "assert 'MultiFlag' in df_sample.columns, \"df_sample must have MultiFlag\"\n",
        "\n",
        "# ======================================\n",
        "# Configuration\n",
        "# ======================================\n",
        "EXPECTED_LABELS_FULL = ['N', 'Z', 'P', 'F']     # final label order\n",
        "DENIED_LABELS = ['Z', 'P', 'F']                 # Stage B labels\n",
        "f1_floor_A = 0.90\n",
        "f1_floor_B = 0.80\n",
        "drop_dt = ['ServiceDt', 'ClaimBillDate', 'lastActDt']  # raw datetimes to drop\n",
        "# Target share for class 'F' in Stage B training AFTER oversampling.\n",
        "# Example: 0.40 means ~40% of Stage-B training rows will be 'F'.\n",
        "F_TARGET_RATIO_B = 0.40   # tune 0.25–0.50 based on validation\n",
        "\n",
        "# Safety: locate the class index for 'F' in Stage B\n",
        "F_CLASS_IDX_B = DENIED_LABELS.index('F')  # expected 2\n",
        "# ======================================\n",
        "# Helpers\n",
        "# ======================================\n",
        "def evaluate_predictions(y_true, y_pred_or_proba, class_names=None, thresholds=None):\n",
        "    \"\"\"Supports hard labels (1D int) or proba matrix (n,K) with optional per-class thresholds.\"\"\"\n",
        "    y_true = np.asarray(y_true).astype(int)\n",
        "    arr = np.asarray(y_pred_or_proba)\n",
        "\n",
        "    # Case 1: already hard labels\n",
        "    if arr.ndim == 1 and np.issubdtype(arr.dtype, np.integer):\n",
        "        y_pred = arr\n",
        "\n",
        "    # Case 2: probability matrix -> hard labels\n",
        "    elif arr.ndim == 2:\n",
        "        proba = arr.astype(float)\n",
        "        n_classes = proba.shape[1]\n",
        "        if thresholds is None:\n",
        "            y_pred = proba.argmax(axis=1)\n",
        "        else:\n",
        "            thr = np.asarray(thresholds, dtype=float)\n",
        "            if thr.shape != (n_classes,):\n",
        "                raise ValueError(f\"thresholds must have shape {(n_classes,)}, got {thr.shape}\")\n",
        "            over = proba >= thr                 # (n, K)\n",
        "            any_over = over.any(axis=1)         # (n,)\n",
        "            scores_over = np.where(over, proba, -np.inf)\n",
        "            y_pred = scores_over.argmax(axis=1)\n",
        "            no_over = ~any_over\n",
        "            if np.any(no_over):\n",
        "                y_pred[no_over] = proba[no_over].argmax(axis=1)\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported input for y_pred_or_proba.\")\n",
        "\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    metrics = {\n",
        "        \"accuracy\": accuracy_score(y_true, y_pred),\n",
        "        \"f1_macro\": f1_score(y_true, y_pred, average=\"macro\", zero_division=0),\n",
        "        \"f1_weighted\": f1_score(y_true, y_pred, average=\"weighted\", zero_division=0),\n",
        "    }\n",
        "    # Optional: macro PR-AUC (OvR) if probabilities were provided; skip here for speed.\n",
        "    rep = classification_report(\n",
        "        y_true, y_pred,\n",
        "        target_names=class_names if class_names is not None else None,\n",
        "        digits=4, zero_division=0\n",
        "    )\n",
        "    return cm, metrics, rep\n",
        "\n",
        "def print_results(model_name, cm, metrics, clsrep):\n",
        "    print(f\"=== {model_name} RESULTS ===\")\n",
        "    print(\"Confusion Matrix (rows=true, cols=pred):\")\n",
        "    print(cm)\n",
        "    print(\"Metrics:\")\n",
        "    for k, v in metrics.items():\n",
        "        try:\n",
        "            print(f\"  {k}: {v:.5f}\")\n",
        "        except Exception:\n",
        "            print(f\"  {k}: {v}\")\n",
        "    print(\"Classification Report:\\n\", clsrep)\n",
        "\n",
        "def threshold_for_f1(y_true, proba, f1_floor=0.90, grid=None):\n",
        "    \"\"\"Pick smallest threshold meeting F1>=floor; else return best-F1 threshold.\"\"\"\n",
        "    y_true = np.asarray(y_true).astype(int)\n",
        "    proba  = np.asarray(proba).ravel()\n",
        "    if grid is None:\n",
        "        grid = np.linspace(0.01, 0.99, 99)\n",
        "    best_thr, best_f1, chosen = 0.5, -1.0, None\n",
        "    for thr in grid:\n",
        "        pred = (proba >= thr).astype(int)\n",
        "        f1 = f1_score(y_true, pred, zero_division=0)\n",
        "        if f1 >= f1_floor and chosen is None:\n",
        "            chosen = float(thr)\n",
        "        if f1 > best_f1:\n",
        "            best_f1, best_thr = f1, float(thr)\n",
        "    return chosen if chosen is not None else best_thr\n",
        "\n",
        "def ovr_thresholds_for_f1(y_true, proba, f1_floor=0.80, grid=None):\n",
        "    \"\"\"Per-class thresholds (OvR) using F1>=floor; else return each class's best-F1 thr.\"\"\"\n",
        "    y_true = np.asarray(y_true).astype(int)\n",
        "    proba  = np.asarray(proba)\n",
        "    K = proba.shape[1]\n",
        "    if grid is None:\n",
        "        grid = np.linspace(0.01, 0.99, 99)\n",
        "    thrs = np.full(K, 0.5, dtype=float)\n",
        "    for k in range(K):\n",
        "        y_bin = (y_true == k).astype(int)\n",
        "        if y_bin.sum() == 0:\n",
        "            thrs[k] = 0.5; continue\n",
        "        pk = proba[:, k]\n",
        "        best_thr, best_f1, chosen = 0.5, -1.0, None\n",
        "        for thr in grid:\n",
        "            pred = (pk >= thr).astype(int)\n",
        "            f1 = f1_score(y_bin, pred, zero_division=0)\n",
        "            if f1 >= f1_floor and chosen is None:\n",
        "                chosen = float(thr)\n",
        "            if f1 > best_f1:\n",
        "                best_f1, best_thr = f1, float(thr)\n",
        "        thrs[k] = chosen if chosen is not None else best_thr\n",
        "    return thrs\n",
        "\n",
        "\n",
        "def make_class_weights(y_arr):\n",
        "    classes, counts = np.unique(y_arr, return_counts=True)\n",
        "    n, k = len(y_arr), len(classes)\n",
        "    return {int(c): float(n / (k * cnt)) for c, cnt in zip(classes, counts)}\n",
        "\n",
        "def align_to_training_native(df_raw: pd.DataFrame, feature_cols: list, cat_dtype_map: dict) -> pd.DataFrame:\n",
        "    \"\"\"Align columns and categorical vocab exactly to training spec.\"\"\"\n",
        "    dfX = df_raw.copy()\n",
        "    # Drop targets if present\n",
        "    for tgt in ('MultiFlag', 'DenialFlag'):\n",
        "        if tgt in dfX.columns:\n",
        "            dfX = dfX.drop(columns=[tgt])\n",
        "    # Add missing features\n",
        "    for c in feature_cols:\n",
        "        if c not in dfX.columns:\n",
        "            if c in cat_dtype_map:\n",
        "                dfX[c] = pd.Series(pd.Categorical([None]*len(dfX), dtype=_CatDType(cat_dtype_map[c])))\n",
        "            else:\n",
        "                dfX[c] = 0\n",
        "    # Enforce categorical vocab\n",
        "    for c, cats in cat_dtype_map.items():\n",
        "        if c in dfX.columns:\n",
        "            dfX[c] = dfX[c].astype(_CatDType(cats))\n",
        "    # Reorder\n",
        "    return dfX[feature_cols]\n",
        "\n",
        "def oversample_class_to_ratio(X, y, target_class: int, target_ratio: float, random_state: int = 123):\n",
        "    \"\"\"\n",
        "    Duplicate rows of `target_class` with replacement so that\n",
        "    target_class / (total_after_oversample) ≈ target_ratio.\n",
        "    Validation/test must NEVER pass through here.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    X : pd.DataFrame\n",
        "    y : array-like of shape (n,)\n",
        "    target_class : int (e.g., 2 for 'F' when DENIED_LABELS=['Z','P','F'])\n",
        "    target_ratio : float in (0,1), desired final share of target_class\n",
        "    random_state : int\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    X_os : pd.DataFrame\n",
        "    y_os : np.ndarray\n",
        "    \"\"\"\n",
        "    y_arr = np.asarray(y).astype(int)\n",
        "    n = len(y_arr)\n",
        "    if n == 0:\n",
        "        return X.copy(), y_arr.copy()\n",
        "\n",
        "    idx_tc = np.flatnonzero(y_arr == target_class)\n",
        "    if len(idx_tc) == 0:\n",
        "        # Nothing to oversample; return unchanged\n",
        "        return X.copy(), y_arr.copy()\n",
        "\n",
        "    # Current counts\n",
        "    n_tc = len(idx_tc)\n",
        "    n_non = n - n_tc\n",
        "    # If current ratio already >= target, skip\n",
        "    curr_ratio = n_tc / max(1, n)\n",
        "    if curr_ratio >= target_ratio:\n",
        "        # Shuffle for good measure\n",
        "        rng = np.random.default_rng(random_state)\n",
        "        order = rng.permutation(n)\n",
        "        return X.iloc[order].copy(), y_arr[order].copy()\n",
        "\n",
        "    # Desired total target count after oversampling:\n",
        "    # Let F' be final target count; with non-target fixed at n_non:\n",
        "    # F' / (n_non + F') = target_ratio -> F' = (target_ratio / (1 - target_ratio)) * n_non\n",
        "    target_count = int(np.ceil((target_ratio / (1.0 - target_ratio)) * n_non))\n",
        "    add_needed = max(0, target_count - n_tc)\n",
        "    if add_needed == 0:\n",
        "        rng = np.random.default_rng(random_state)\n",
        "        order = rng.permutation(n)\n",
        "        return X.iloc[order].copy(), y_arr[order].copy()\n",
        "\n",
        "    rng = np.random.default_rng(random_state)\n",
        "    add_idx = rng.choice(idx_tc, size=add_needed, replace=True)\n",
        "\n",
        "    # Concatenate originals + sampled duplicates\n",
        "    X_os = pd.concat([X, X.iloc[add_idx]], axis=0)\n",
        "    y_os = np.concatenate([y_arr, y_arr[add_idx]], axis=0)\n",
        "\n",
        "    # Final shuffle\n",
        "    order = rng.permutation(len(y_os))\n",
        "    return X_os.iloc[order].copy(), y_os[order].copy()\n",
        "\n",
        "# ======================================\n",
        "# CLEAN + TARGETS\n",
        "# ======================================\n",
        "# Keep a working copy, drop raw datetime columns\n",
        "df_work = df_sample.copy()\n",
        "df_work = df_work.drop(columns=drop_dt, errors='ignore')\n",
        "\n",
        "# Clean target\n",
        "df_work['MultiFlag'] = (\n",
        "    df_work['MultiFlag'].astype(str).str.strip().replace('', np.nan)\n",
        ")\n",
        "df_work = df_work[df_work['MultiFlag'].notna()].copy()\n",
        "df_work['MultiFlag'] = pd.Categorical(df_work['MultiFlag'], categories=EXPECTED_LABELS_FULL, ordered=False)\n",
        "\n",
        "# Cast predictor categoricals to 'category'\n",
        "categorical_cols = [\n",
        "    c for c in df_work.select_dtypes(include=['object', 'category']).columns\n",
        "    if c != 'MultiFlag'\n",
        "]\n",
        "if 'ClientID' in df_work.columns and 'ClientID' not in categorical_cols:\n",
        "    categorical_cols.append('ClientID')\n",
        "for c in categorical_cols:\n",
        "    df_work[c] = df_work[c].astype('category')\n",
        "\n",
        "# Build X / y (full multi labels)\n",
        "X_all = df_work.drop(columns=['MultiFlag'])\n",
        "y_full_codes = df_work['MultiFlag'].cat.codes  # 0..3 -> ['N','Z','P','F']\n",
        "\n",
        "class_map_full = dict(enumerate(df_work['MultiFlag'].cat.categories))   # {0:'N',1:'Z',2:'P',3:'F'}\n",
        "inv_map_full   = {v:k for k,v in class_map_full.items()}\n",
        "code_N = inv_map_full['N']\n",
        "class_names_full = [class_map_full[i] for i in range(len(class_map_full))]\n",
        "\n",
        "# Lock category vocab\n",
        "cat_dtype_map = {c: list(df_work[c].cat.categories) for c in categorical_cols}\n",
        "\n",
        "# ======================================\n",
        "# SPLIT + enforce category vocab\n",
        "# ======================================\n",
        "X_train_full, X_test, y_full_train_full, y_full_test = train_test_split(\n",
        "    X_all, y_full_codes, test_size=0.20, random_state=123, stratify=y_full_codes\n",
        ")\n",
        "X_train, X_val, y_full_train, y_full_val = train_test_split(\n",
        "    X_train_full, y_full_train_full, test_size=0.25, random_state=123, stratify=y_full_train_full\n",
        ")\n",
        "\n",
        "def _enforce_cats(dfX):\n",
        "    for c, cats in cat_dtype_map.items():\n",
        "        if c in dfX.columns:\n",
        "            dfX[c] = dfX[c].astype(_CatDType(cats))\n",
        "    return dfX\n",
        "\n",
        "X_train = _enforce_cats(X_train)\n",
        "X_val   = _enforce_cats(X_val)\n",
        "X_test  = _enforce_cats(X_test)\n",
        "\n",
        "# Show class distributions\n",
        "def _show_dist(name, y_arr, cmap):\n",
        "    vc = pd.Series(y_arr).value_counts().sort_index()\n",
        "    print(f\"{name}:\", {cmap[int(k)]: int(v) for k, v in vc.items()})\n",
        "\n",
        "_show_dist(\"Train full labels\", y_full_train, class_map_full)\n",
        "_show_dist(\"Val full labels\",   y_full_val,   class_map_full)\n",
        "_show_dist(\"Test full labels\",  y_full_test,  class_map_full)\n",
        "\n",
        "# ======================================\n",
        "# Derive Stage A (binary) and Stage B (multi denied)\n",
        "# ======================================\n",
        "# Stage A: Denied vs NotDenied\n",
        "yA_train = (y_full_train != code_N).astype(int)\n",
        "yA_val   = (y_full_val   != code_N).astype(int)\n",
        "yA_test  = (y_full_test  != code_N).astype(int)\n",
        "\n",
        "# Stage B: only denied rows; labels = Z,P,F -> codes 0..2 in DENIED_LABELS order\n",
        "maskA_train_den = (yA_train == 1)\n",
        "maskA_val_den   = (yA_val   == 1)\n",
        "maskA_test_den  = (yA_test  == 1)\n",
        "\n",
        "yB_train_labels = pd.Series(y_full_train, index=X_train.index).map(lambda c: class_map_full[int(c)]).loc[maskA_train_den]\n",
        "yB_val_labels   = pd.Series(y_full_val,   index=X_val.index).map(lambda c: class_map_full[int(c)]).loc[maskA_val_den]\n",
        "yB_test_labels  = pd.Series(y_full_test,  index=X_test.index).map(lambda c: class_map_full[int(c)]).loc[maskA_test_den]\n",
        "\n",
        "yB_train = pd.Categorical(yB_train_labels, categories=DENIED_LABELS).codes\n",
        "yB_val   = pd.Categorical(yB_val_labels,   categories=DENIED_LABELS).codes\n",
        "yB_test  = pd.Categorical(yB_test_labels,  categories=DENIED_LABELS).codes\n",
        "\n",
        "X_train_B = X_train.loc[maskA_train_den].copy()\n",
        "X_val_B   = X_val.loc[maskA_val_den].copy()\n",
        "X_test_B  = X_test.loc[maskA_test_den].copy()\n",
        "\n",
        "# Assemble tr+val pools\n",
        "X_trval_A = pd.concat([X_train, X_val], axis=0)\n",
        "y_trval_A = pd.concat([pd.Series(yA_train, index=X_train.index),\n",
        "                       pd.Series(yA_val,   index=X_val.index)], axis=0)\n",
        "X_trval_A = _enforce_cats(X_trval_A)\n",
        "\n",
        "X_trval_B = pd.concat([X_train_B, X_val_B], axis=0)\n",
        "y_trval_B = pd.concat([pd.Series(yB_train, index=X_train_B.index),\n",
        "                       pd.Series(yB_val,   index=X_val_B.index)], axis=0)\n",
        "X_trval_B = _enforce_cats(X_trval_B)\n",
        "\n",
        "# Class/sample weights\n",
        "cw_A  = make_class_weights(yA_train)\n",
        "sw_A_train = pd.Series(yA_train).map(cw_A).to_numpy()\n",
        "sw_A_val   = pd.Series(yA_val).map(cw_A).to_numpy()\n",
        "sw_A_trval = pd.Series(y_trval_A).map(cw_A).to_numpy()\n",
        "\n",
        "cw_B  = make_class_weights(yB_train)\n",
        "sw_B_train = pd.Series(yB_train).map(cw_B).to_numpy()\n",
        "sw_B_val   = pd.Series(yB_val).map(cw_B).to_numpy()\n",
        "sw_B_trval = pd.Series(y_trval_B).map(cw_B).to_numpy()\n",
        "\n",
        "print(\"Stage A class weights:\", cw_A)\n",
        "print(\"Stage B class weights:\", {DENIED_LABELS[k]: v for k, v in cw_B.items()})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-09-13 20:20:55,211] A new study created in memory with name: no-name-1d0fb7d3-c406-4f42-a4f3-cc0c07f71eca\n",
            "[I 2025-09-13 20:44:00,805] Trial 0 finished with value: 0.9114492410846406 and parameters: {'eta': 0.14489833359973686, 'max_depth': 8, 'min_child_weight': 2.115927435351363, 'subsample': 0.9316564160887764, 'colsample_bytree': 0.7160678146582435, 'reg_lambda': 12.344998218176437, 'reg_alpha': 0.019389406313206712, 'gamma': 2.147518189419446, 'grow_policy': 'lossguide', 'max_bin': 64, 'max_cat_to_onehot': 10}. Best is trial 0 with value: 0.9114492410846406.\n",
            "[I 2025-09-13 21:08:08,965] Trial 1 finished with value: 0.9097811385648663 and parameters: {'eta': 0.19319358912594076, 'max_depth': 7, 'min_child_weight': 1.677834150039486, 'subsample': 0.502371283293334, 'colsample_bytree': 0.732963601126652, 'reg_lambda': 42.13895959885411, 'reg_alpha': 1.2904163240330186, 'gamma': 1.2564689356103675, 'grow_policy': 'depthwise', 'max_bin': 128, 'max_cat_to_onehot': 1}. Best is trial 0 with value: 0.9114492410846406.\n",
            "[I 2025-09-13 21:32:59,222] Trial 2 finished with value: 0.9023948888613191 and parameters: {'eta': 0.029010891245919403, 'max_depth': 4, 'min_child_weight': 4.372744102510131, 'subsample': 0.9290354732242485, 'colsample_bytree': 0.5437726413994564, 'reg_lambda': 1.8803964309806362, 'reg_alpha': 2.3110375458829777, 'gamma': 8.008831221338038, 'grow_policy': 'lossguide', 'max_bin': 64, 'max_cat_to_onehot': 1}. Best is trial 0 with value: 0.9114492410846406.\n",
            "[I 2025-09-13 22:01:35,057] Trial 3 finished with value: 0.9022786269839627 and parameters: {'eta': 0.12487592071339998, 'max_depth': 6, 'min_child_weight': 12.242352037917616, 'subsample': 0.8410885415230538, 'colsample_bytree': 0.8282042201438735, 'reg_lambda': 8.37639792583033e-06, 'reg_alpha': 3.78001746097589e-05, 'gamma': 6.779041207644886, 'grow_policy': 'depthwise', 'max_bin': 128, 'max_cat_to_onehot': 2}. Best is trial 0 with value: 0.9114492410846406.\n",
            "[I 2025-09-13 22:30:01,566] Trial 4 finished with value: 0.910958773832008 and parameters: {'eta': 0.07366663123449015, 'max_depth': 8, 'min_child_weight': 3.0374276209371596, 'subsample': 0.8022188103394671, 'colsample_bytree': 0.7162032011078381, 'reg_lambda': 0.00017540040960334273, 'reg_alpha': 7.463093943975065e-06, 'gamma': 0.9693776894292583, 'grow_policy': 'lossguide', 'max_bin': 256, 'max_cat_to_onehot': 2}. Best is trial 0 with value: 0.9114492410846406.\n",
            "[I 2025-09-13 22:57:20,972] Trial 5 finished with value: 0.9005909888843409 and parameters: {'eta': 0.03220108051138717, 'max_depth': 4, 'min_child_weight': 2.134682408548182, 'subsample': 0.5045903384714902, 'colsample_bytree': 0.6304813131816438, 'reg_lambda': 4.880530911704962e-06, 'reg_alpha': 0.0010452380949528216, 'gamma': 8.199652131757585, 'grow_policy': 'depthwise', 'max_bin': 64, 'max_cat_to_onehot': 1}. Best is trial 0 with value: 0.9114492410846406.\n",
            "[I 2025-09-13 23:37:51,792] Trial 6 finished with value: 0.8993476768827 and parameters: {'eta': 0.0396784170401236, 'max_depth': 9, 'min_child_weight': 14.683662659375672, 'subsample': 0.7283310133420932, 'colsample_bytree': 0.8903358284339724, 'reg_lambda': 0.010854218294960425, 'reg_alpha': 0.004598709680186973, 'gamma': 0.8819640448867938, 'grow_policy': 'depthwise', 'max_bin': 256, 'max_cat_to_onehot': 1}. Best is trial 0 with value: 0.9114492410846406.\n",
            "[I 2025-09-13 23:57:10,379] Trial 7 finished with value: 0.9068173425718143 and parameters: {'eta': 0.10687953668802118, 'max_depth': 9, 'min_child_weight': 4.695509622751351, 'subsample': 0.951213356257306, 'colsample_bytree': 0.9325216392356095, 'reg_lambda': 1.4780558189530155e-06, 'reg_alpha': 0.0002426500156415562, 'gamma': 8.937456231299942, 'grow_policy': 'lossguide', 'max_bin': 64, 'max_cat_to_onehot': 10}. Best is trial 0 with value: 0.9114492410846406.\n",
            "[I 2025-09-14 00:29:45,101] Trial 8 finished with value: 0.905268429028391 and parameters: {'eta': 0.021590461295682675, 'max_depth': 7, 'min_child_weight': 7.320756370762906, 'subsample': 0.9792350394830802, 'colsample_bytree': 0.6101670878276659, 'reg_lambda': 0.0009428620225375308, 'reg_alpha': 0.002252932884396953, 'gamma': 2.7786436176451588, 'grow_policy': 'depthwise', 'max_bin': 64, 'max_cat_to_onehot': 2}. Best is trial 0 with value: 0.9114492410846406.\n",
            "[I 2025-09-14 00:36:47,746] Trial 9 finished with value: 0.90337306697676 and parameters: {'eta': 0.05144499926237768, 'max_depth': 8, 'min_child_weight': 6.329518604406253, 'subsample': 0.9970756640999533, 'colsample_bytree': 0.7030505077655064, 'reg_lambda': 0.1576225765140297, 'reg_alpha': 0.8974159012278339, 'gamma': 9.59280548221135, 'grow_policy': 'lossguide', 'max_bin': 128, 'max_cat_to_onehot': 2}. Best is trial 0 with value: 0.9114492410846406.\n",
            "[I 2025-09-14 00:58:20,197] Trial 10 finished with value: 0.9112326613470684 and parameters: {'eta': 0.18694768316086605, 'max_depth': 12, 'min_child_weight': 1.1754559977367287, 'subsample': 0.6953654865997481, 'colsample_bytree': 0.829659683312801, 'reg_lambda': 43.01921966315958, 'reg_alpha': 0.06776462678856783, 'gamma': 3.953487218160367, 'grow_policy': 'lossguide', 'max_bin': 64, 'max_cat_to_onehot': 10}. Best is trial 0 with value: 0.9114492410846406.\n",
            "[I 2025-09-14 01:21:43,535] Trial 11 finished with value: 0.9113874760853996 and parameters: {'eta': 0.1990958762939571, 'max_depth': 12, 'min_child_weight': 1.0714400842533331, 'subsample': 0.6609005818037337, 'colsample_bytree': 0.8186154937540275, 'reg_lambda': 35.605882630843894, 'reg_alpha': 0.0561003585163862, 'gamma': 4.1770249755366065, 'grow_policy': 'lossguide', 'max_bin': 64, 'max_cat_to_onehot': 10}. Best is trial 0 with value: 0.9114492410846406.\n",
            "[I 2025-09-14 01:47:37,913] Trial 12 finished with value: 0.9119217248320265 and parameters: {'eta': 0.11734518086540702, 'max_depth': 12, 'min_child_weight': 1.035170546378782, 'subsample': 0.620202154498388, 'colsample_bytree': 0.8124517141621229, 'reg_lambda': 0.8887358638852253, 'reg_alpha': 0.04140557741325877, 'gamma': 5.50506552261402, 'grow_policy': 'lossguide', 'max_bin': 64, 'max_cat_to_onehot': 10}. Best is trial 12 with value: 0.9119217248320265.\n",
            "[I 2025-09-14 02:21:12,024] Trial 13 finished with value: 0.9105816897081587 and parameters: {'eta': 0.1061952156200656, 'max_depth': 10, 'min_child_weight': 2.051234367307293, 'subsample': 0.6215532782708822, 'colsample_bytree': 0.993136719133524, 'reg_lambda': 0.5917796834059348, 'reg_alpha': 0.05024057088341976, 'gamma': 5.889250772849482, 'grow_policy': 'lossguide', 'max_bin': 64, 'max_cat_to_onehot': 10}. Best is trial 12 with value: 0.9119217248320265.\n",
            "[I 2025-09-14 02:47:08,243] Trial 14 finished with value: 0.89494895848179 and parameters: {'eta': 0.07558475826467607, 'max_depth': 11, 'min_child_weight': 1.464772609892551, 'subsample': 0.5983274640213821, 'colsample_bytree': 0.78242602754239, 'reg_lambda': 0.07224026784185679, 'reg_alpha': 16.960170983617356, 'gamma': 2.9108012467343287, 'grow_policy': 'lossguide', 'max_bin': 64, 'max_cat_to_onehot': 10}. Best is trial 12 with value: 0.9119217248320265.\n",
            "[I 2025-09-14 03:18:42,591] Trial 15 finished with value: 0.9108966555466209 and parameters: {'eta': 0.13455544643028136, 'max_depth': 10, 'min_child_weight': 2.766978901104531, 'subsample': 0.8602092539825438, 'colsample_bytree': 0.6476357071910644, 'reg_lambda': 2.914655762997032, 'reg_alpha': 0.013731640513933748, 'gamma': 5.58397382144928, 'grow_policy': 'lossguide', 'max_bin': 256, 'max_cat_to_onehot': 10}. Best is trial 12 with value: 0.9119217248320265.\n",
            "[I 2025-09-14 03:54:03,906] Trial 16 finished with value: 0.9117877224772512 and parameters: {'eta': 0.08660857818107269, 'max_depth': 6, 'min_child_weight': 1.0310723189357576, 'subsample': 0.775375195239936, 'colsample_bytree': 0.9125913449901988, 'reg_lambda': 4.499829474614849, 'reg_alpha': 0.23214101312581809, 'gamma': 2.5398332622698128, 'grow_policy': 'lossguide', 'max_bin': 64, 'max_cat_to_onehot': 10}. Best is trial 12 with value: 0.9119217248320265.\n",
            "[I 2025-09-14 04:24:56,521] Trial 17 finished with value: 0.9087728856037429 and parameters: {'eta': 0.0852008126322107, 'max_depth': 5, 'min_child_weight': 1.211553278217931, 'subsample': 0.7804853262903143, 'colsample_bytree': 0.9012761532710373, 'reg_lambda': 0.020321657292561725, 'reg_alpha': 0.2188007565082486, 'gamma': 4.388775526934095, 'grow_policy': 'lossguide', 'max_bin': 64, 'max_cat_to_onehot': 10}. Best is trial 12 with value: 0.9119217248320265.\n",
            "[I 2025-09-14 04:59:47,433] Trial 18 finished with value: 0.8944901730259328 and parameters: {'eta': 0.05625594341636087, 'max_depth': 6, 'min_child_weight': 1.0125871624041838, 'subsample': 0.559397194657148, 'colsample_bytree': 0.9986328323006596, 'reg_lambda': 0.39413120330200374, 'reg_alpha': 19.272035396843187, 'gamma': 0.012875738381099566, 'grow_policy': 'lossguide', 'max_bin': 128, 'max_cat_to_onehot': 10}. Best is trial 12 with value: 0.9119217248320265.\n",
            "[I 2025-09-14 05:33:37,039] Trial 19 finished with value: 0.9089520572105734 and parameters: {'eta': 0.09461879919828659, 'max_depth': 6, 'min_child_weight': 3.1363118218775616, 'subsample': 0.7269014869370664, 'colsample_bytree': 0.8759364899610215, 'reg_lambda': 0.0017105995082818347, 'reg_alpha': 0.3782997390389665, 'gamma': 5.901817608276669, 'grow_policy': 'lossguide', 'max_bin': 256, 'max_cat_to_onehot': 10}. Best is trial 12 with value: 0.9119217248320265.\n",
            "[I 2025-09-14 06:02:14,954] Trial 20 finished with value: 0.899466640499301 and parameters: {'eta': 0.06778371735795104, 'max_depth': 5, 'min_child_weight': 10.49463901917296, 'subsample': 0.6649318560114648, 'colsample_bytree': 0.9394834523846033, 'reg_lambda': 2.9908611418378146, 'reg_alpha': 0.00026688063388574407, 'gamma': 7.1467397103362496, 'grow_policy': 'lossguide', 'max_bin': 64, 'max_cat_to_onehot': 10}. Best is trial 12 with value: 0.9119217248320265.\n",
            "[I 2025-09-14 06:26:24,780] Trial 21 finished with value: 0.9120056865542331 and parameters: {'eta': 0.1451320461918438, 'max_depth': 7, 'min_child_weight': 1.6628128514592107, 'subsample': 0.8786766803057895, 'colsample_bytree': 0.7687121940264241, 'reg_lambda': 3.733250157703692, 'reg_alpha': 0.012858302251968387, 'gamma': 2.6279872681950875, 'grow_policy': 'lossguide', 'max_bin': 64, 'max_cat_to_onehot': 10}. Best is trial 21 with value: 0.9120056865542331.\n",
            "[I 2025-09-14 06:52:05,544] Trial 22 finished with value: 0.9119774281777346 and parameters: {'eta': 0.15428908269589225, 'max_depth': 7, 'min_child_weight': 1.526500790937668, 'subsample': 0.8791647484753268, 'colsample_bytree': 0.7883817395063161, 'reg_lambda': 4.893443429063728, 'reg_alpha': 0.009949726448500954, 'gamma': 3.2866074826925735, 'grow_policy': 'lossguide', 'max_bin': 64, 'max_cat_to_onehot': 10}. Best is trial 21 with value: 0.9120056865542331.\n",
            "[I 2025-09-14 07:24:09,229] Trial 23 finished with value: 0.9115952015476676 and parameters: {'eta': 0.1568114624700428, 'max_depth': 7, 'min_child_weight': 1.5710167390102878, 'subsample': 0.8823013991891232, 'colsample_bytree': 0.7847731747295758, 'reg_lambda': 0.871688117390785, 'reg_alpha': 0.007659938010816459, 'gamma': 3.3759838418355677, 'grow_policy': 'lossguide', 'max_bin': 64, 'max_cat_to_onehot': 10}. Best is trial 21 with value: 0.9120056865542331.\n",
            "[I 2025-09-14 07:54:49,913] Trial 24 finished with value: 0.9116645783982179 and parameters: {'eta': 0.11968095774207017, 'max_depth': 9, 'min_child_weight': 1.4408546007363903, 'subsample': 0.8977633803013177, 'colsample_bytree': 0.7743864926928813, 'reg_lambda': 0.06727782260925147, 'reg_alpha': 0.0007518545987772, 'gamma': 4.9104577537664955, 'grow_policy': 'lossguide', 'max_bin': 64, 'max_cat_to_onehot': 10}. Best is trial 21 with value: 0.9120056865542331.\n",
            "[I 2025-09-14 08:16:03,575] Trial 25 finished with value: 0.911276334861358 and parameters: {'eta': 0.16024950371703048, 'max_depth': 7, 'min_child_weight': 1.8932010162222304, 'subsample': 0.8177593901887324, 'colsample_bytree': 0.6697388413134342, 'reg_lambda': 8.030528565623884, 'reg_alpha': 0.00012406714617183378, 'gamma': 1.9251953544541425, 'grow_policy': 'lossguide', 'max_bin': 64, 'max_cat_to_onehot': 10}. Best is trial 21 with value: 0.9120056865542331.\n",
            "[I 2025-09-14 08:38:48,528] Trial 26 finished with value: 0.9117448142843635 and parameters: {'eta': 0.1633399266723048, 'max_depth': 8, 'min_child_weight': 1.3588128636172407, 'subsample': 0.863985507809143, 'colsample_bytree': 0.8544787641577782, 'reg_lambda': 0.20667718995868634, 'reg_alpha': 1.9461855047112693e-06, 'gamma': 3.4684442442517187, 'grow_policy': 'depthwise', 'max_bin': 64, 'max_cat_to_onehot': 10}. Best is trial 21 with value: 0.9120056865542331.\n",
            "[I 2025-09-14 09:11:50,063] Trial 27 finished with value: 0.9102783562443915 and parameters: {'eta': 0.11027821856442195, 'max_depth': 10, 'min_child_weight': 2.649229554879455, 'subsample': 0.9014390688695452, 'colsample_bytree': 0.7657755760145785, 'reg_lambda': 12.351280099904502, 'reg_alpha': 0.017390847753752655, 'gamma': 5.0081484493589645, 'grow_policy': 'lossguide', 'max_bin': 64, 'max_cat_to_onehot': 10}. Best is trial 21 with value: 0.9120056865542331.\n",
            "[I 2025-09-14 09:40:41,743] Trial 28 finished with value: 0.9074471666526251 and parameters: {'eta': 0.13736825799277283, 'max_depth': 5, 'min_child_weight': 3.7308918682966237, 'subsample': 0.7558897672366508, 'colsample_bytree': 0.8032515039472997, 'reg_lambda': 0.7724786920863538, 'reg_alpha': 0.0030118883012110343, 'gamma': 6.858571558201742, 'grow_policy': 'lossguide', 'max_bin': 256, 'max_cat_to_onehot': 2}. Best is trial 21 with value: 0.9120056865542331.\n",
            "[I 2025-09-14 09:55:12,937] Trial 29 finished with value: 0.9119205438813159 and parameters: {'eta': 0.14611043403799812, 'max_depth': 11, 'min_child_weight': 2.3072687513763075, 'subsample': 0.9374714436111913, 'colsample_bytree': 0.6895343275843011, 'reg_lambda': 10.121438071451484, 'reg_alpha': 0.02716925746282204, 'gamma': 1.8542226968993045, 'grow_policy': 'lossguide', 'max_bin': 128, 'max_cat_to_onehot': 1}. Best is trial 21 with value: 0.9120056865542331.\n",
            "[I 2025-09-14 10:21:47,172] A new study created in memory with name: no-name-aee2a28b-9a01-4f7c-b28c-29a7cba7120a\n",
            "[I 2025-09-14 10:36:52,934] Trial 0 finished with value: 0.8660460685761453 and parameters: {'eta': 0.031059975207586337, 'max_depth': 4, 'min_child_weight': 6.592599864880579, 'subsample': 0.9836862664609702, 'colsample_bytree': 0.556008824114349, 'reg_lambda': 1.8858621653779557, 'reg_alpha': 0.0494096511547221, 'gamma': 7.301261052102397, 'grow_policy': 'lossguide', 'max_bin': 256, 'max_cat_to_onehot': 10}. Best is trial 0 with value: 0.8660460685761453.\n",
            "[I 2025-09-14 11:08:35,913] Trial 1 finished with value: 0.8784855434915594 and parameters: {'eta': 0.031569771949860666, 'max_depth': 8, 'min_child_weight': 6.19789870490519, 'subsample': 0.9069541409774748, 'colsample_bytree': 0.6843780584634543, 'reg_lambda': 12.108512415783542, 'reg_alpha': 0.015080543574765029, 'gamma': 1.7811064395256215, 'grow_policy': 'lossguide', 'max_bin': 64, 'max_cat_to_onehot': 10}. Best is trial 1 with value: 0.8784855434915594.\n",
            "[I 2025-09-14 11:32:32,294] Trial 2 finished with value: 0.88383418180128 and parameters: {'eta': 0.06871563380105562, 'max_depth': 10, 'min_child_weight': 1.7054412825171912, 'subsample': 0.734271628344974, 'colsample_bytree': 0.8773479952039458, 'reg_lambda': 1.836657947139751e-05, 'reg_alpha': 0.0011872541132092657, 'gamma': 1.059418536287965, 'grow_policy': 'lossguide', 'max_bin': 64, 'max_cat_to_onehot': 2}. Best is trial 2 with value: 0.88383418180128.\n",
            "[I 2025-09-14 11:52:42,116] Trial 3 finished with value: 0.8820460370322685 and parameters: {'eta': 0.04157613640072179, 'max_depth': 8, 'min_child_weight': 4.722315796068653, 'subsample': 0.9547651919320286, 'colsample_bytree': 0.7588984516599476, 'reg_lambda': 0.041253967014190374, 'reg_alpha': 0.01601480570779499, 'gamma': 1.1750484233742498, 'grow_policy': 'depthwise', 'max_bin': 256, 'max_cat_to_onehot': 1}. Best is trial 2 with value: 0.88383418180128.\n",
            "[I 2025-09-14 12:24:00,309] Trial 4 finished with value: 0.876953609286273 and parameters: {'eta': 0.04755857331271774, 'max_depth': 6, 'min_child_weight': 1.364446039963818, 'subsample': 0.6449947133523712, 'colsample_bytree': 0.9889558589519167, 'reg_lambda': 10.548778244880793, 'reg_alpha': 0.03424883240885053, 'gamma': 3.267171421083117, 'grow_policy': 'lossguide', 'max_bin': 256, 'max_cat_to_onehot': 10}. Best is trial 2 with value: 0.88383418180128.\n",
            "[I 2025-09-14 12:37:58,363] Trial 5 finished with value: 0.8650988497579803 and parameters: {'eta': 0.07171421153622853, 'max_depth': 8, 'min_child_weight': 14.039380425435922, 'subsample': 0.6512729300173841, 'colsample_bytree': 0.5116001029664605, 'reg_lambda': 0.0026406492291265373, 'reg_alpha': 2.620157193547208e-06, 'gamma': 4.292308831958902, 'grow_policy': 'depthwise', 'max_bin': 256, 'max_cat_to_onehot': 10}. Best is trial 2 with value: 0.88383418180128.\n",
            "[I 2025-09-14 12:44:48,208] Trial 6 finished with value: 0.868881497174001 and parameters: {'eta': 0.17341028490339438, 'max_depth': 4, 'min_child_weight': 3.6238409778097864, 'subsample': 0.8571809184763091, 'colsample_bytree': 0.8893570731867038, 'reg_lambda': 0.004010935693934574, 'reg_alpha': 1.3061950147518788e-06, 'gamma': 9.167598611855453, 'grow_policy': 'depthwise', 'max_bin': 128, 'max_cat_to_onehot': 2}. Best is trial 2 with value: 0.88383418180128.\n",
            "[I 2025-09-14 13:11:10,212] Trial 7 finished with value: 0.8771698257088343 and parameters: {'eta': 0.023780817806446905, 'max_depth': 12, 'min_child_weight': 2.011956130618617, 'subsample': 0.571305464351795, 'colsample_bytree': 0.925570896405318, 'reg_lambda': 0.0016539110414617588, 'reg_alpha': 0.0019381218633914765, 'gamma': 4.163587173907758, 'grow_policy': 'lossguide', 'max_bin': 64, 'max_cat_to_onehot': 1}. Best is trial 2 with value: 0.88383418180128.\n",
            "[I 2025-09-14 13:33:16,625] Trial 8 finished with value: 0.8810030942494814 and parameters: {'eta': 0.06481305425171102, 'max_depth': 8, 'min_child_weight': 3.785918086091315, 'subsample': 0.8804384174639585, 'colsample_bytree': 0.8383427240261963, 'reg_lambda': 0.0017979124679487989, 'reg_alpha': 0.04909216008132093, 'gamma': 2.197079792532466, 'grow_policy': 'lossguide', 'max_bin': 256, 'max_cat_to_onehot': 2}. Best is trial 2 with value: 0.88383418180128.\n",
            "[I 2025-09-14 13:42:13,957] Trial 9 finished with value: 0.8766173017965431 and parameters: {'eta': 0.18620899456494552, 'max_depth': 8, 'min_child_weight': 6.8923851750883705, 'subsample': 0.8592625809716175, 'colsample_bytree': 0.6036054781820251, 'reg_lambda': 0.03415839028467738, 'reg_alpha': 0.0030984962056948982, 'gamma': 3.730759095704003, 'grow_policy': 'lossguide', 'max_bin': 128, 'max_cat_to_onehot': 2}. Best is trial 2 with value: 0.88383418180128.\n",
            "[I 2025-09-14 13:54:29,897] Trial 10 finished with value: 0.868531867815939 and parameters: {'eta': 0.10888846977232745, 'max_depth': 12, 'min_child_weight': 1.042288462866488, 'subsample': 0.7761544218922796, 'colsample_bytree': 0.7857422755546302, 'reg_lambda': 3.827045796639506e-06, 'reg_alpha': 12.415678697768845, 'gamma': 0.1523988933572732, 'grow_policy': 'depthwise', 'max_bin': 64, 'max_cat_to_onehot': 2}. Best is trial 2 with value: 0.88383418180128.\n",
            "[I 2025-09-14 14:16:28,931] Trial 11 finished with value: 0.8844437236245735 and parameters: {'eta': 0.04421048716276285, 'max_depth': 10, 'min_child_weight': 2.2420005811370594, 'subsample': 0.731802535029345, 'colsample_bytree': 0.708108301570513, 'reg_lambda': 1.5342707517231478e-06, 'reg_alpha': 7.069819487139812e-05, 'gamma': 0.16483199524001768, 'grow_policy': 'depthwise', 'max_bin': 64, 'max_cat_to_onehot': 1}. Best is trial 11 with value: 0.8844437236245735.\n",
            "[I 2025-09-14 14:31:58,181] Trial 12 finished with value: 0.8847118297041915 and parameters: {'eta': 0.09584657427611083, 'max_depth': 10, 'min_child_weight': 2.116829979710224, 'subsample': 0.7482314648790066, 'colsample_bytree': 0.6910026311186243, 'reg_lambda': 1.656030360512749e-06, 'reg_alpha': 6.103211728316027e-05, 'gamma': 0.08605246889649792, 'grow_policy': 'depthwise', 'max_bin': 64, 'max_cat_to_onehot': 1}. Best is trial 12 with value: 0.8847118297041915.\n",
            "[I 2025-09-14 14:35:28,347] Trial 13 finished with value: 0.8782418524378303 and parameters: {'eta': 0.10322042230129863, 'max_depth': 10, 'min_child_weight': 2.2095137769048425, 'subsample': 0.7473941965887049, 'colsample_bytree': 0.6607635118158504, 'reg_lambda': 1.1307993526867788e-06, 'reg_alpha': 5.024771838169519e-05, 'gamma': 6.152116421853751, 'grow_policy': 'depthwise', 'max_bin': 64, 'max_cat_to_onehot': 1}. Best is trial 12 with value: 0.8847118297041915.\n",
            "[I 2025-09-14 14:49:30,924] Trial 14 finished with value: 0.8839394721931215 and parameters: {'eta': 0.09645191633343715, 'max_depth': 10, 'min_child_weight': 2.693007777435272, 'subsample': 0.6887454712183203, 'colsample_bytree': 0.6851959222670779, 'reg_lambda': 5.69029057745183e-05, 'reg_alpha': 8.000401912257387e-05, 'gamma': 0.053126337347389765, 'grow_policy': 'depthwise', 'max_bin': 64, 'max_cat_to_onehot': 1}. Best is trial 12 with value: 0.8847118297041915.\n",
            "[I 2025-09-14 15:04:25,928] Trial 15 finished with value: 0.8785007503999246 and parameters: {'eta': 0.0453615042611263, 'max_depth': 11, 'min_child_weight': 2.743169014080163, 'subsample': 0.5286334687836234, 'colsample_bytree': 0.6349459260689984, 'reg_lambda': 9.750335587720172e-05, 'reg_alpha': 4.649162036618252e-05, 'gamma': 2.5149127433274296, 'grow_policy': 'depthwise', 'max_bin': 64, 'max_cat_to_onehot': 1}. Best is trial 12 with value: 0.8847118297041915.\n",
            "[I 2025-09-14 15:13:33,456] Trial 16 finished with value: 0.875231952034224 and parameters: {'eta': 0.13104276359371095, 'max_depth': 6, 'min_child_weight': 1.380529448089488, 'subsample': 0.8008939461916275, 'colsample_bytree': 0.7172670619396997, 'reg_lambda': 1.346377185695392e-05, 'reg_alpha': 0.00026732285664413303, 'gamma': 5.742619630243302, 'grow_policy': 'depthwise', 'max_bin': 64, 'max_cat_to_onehot': 1}. Best is trial 12 with value: 0.8847118297041915.\n",
            "[I 2025-09-14 15:28:03,089] Trial 17 finished with value: 0.8685377742942993 and parameters: {'eta': 0.08451500967480977, 'max_depth': 9, 'min_child_weight': 18.691687630825285, 'subsample': 0.6944028046842206, 'colsample_bytree': 0.7950845646222054, 'reg_lambda': 0.00022901174428289507, 'reg_alpha': 1.109239482933936e-05, 'gamma': 0.08963250977254272, 'grow_policy': 'depthwise', 'max_bin': 128, 'max_cat_to_onehot': 1}. Best is trial 12 with value: 0.8847118297041915.\n",
            "[I 2025-09-14 15:38:55,270] Trial 18 finished with value: 0.8803994518239898 and parameters: {'eta': 0.05209754447652435, 'max_depth': 11, 'min_child_weight': 2.849049997503573, 'subsample': 0.8120926732907006, 'colsample_bytree': 0.5901455475999864, 'reg_lambda': 2.1938518938030158e-06, 'reg_alpha': 0.534992274857591, 'gamma': 2.6618301797808606, 'grow_policy': 'depthwise', 'max_bin': 64, 'max_cat_to_onehot': 1}. Best is trial 12 with value: 0.8847118297041915.\n",
            "[I 2025-09-14 15:43:09,444] Trial 19 finished with value: 0.8710611495293497 and parameters: {'eta': 0.14104200580122506, 'max_depth': 6, 'min_child_weight': 1.0059452225707208, 'subsample': 0.5878056987121941, 'colsample_bytree': 0.7208670455481151, 'reg_lambda': 0.0002794015079908787, 'reg_alpha': 8.735503002870484e-06, 'gamma': 8.404149995746472, 'grow_policy': 'depthwise', 'max_bin': 64, 'max_cat_to_onehot': 1}. Best is trial 12 with value: 0.8847118297041915.\n",
            "[I 2025-09-14 16:08:07,242] Trial 20 finished with value: 0.8824890864052852 and parameters: {'eta': 0.034654381558297434, 'max_depth': 9, 'min_child_weight': 1.572592490324945, 'subsample': 0.6970429846499597, 'colsample_bytree': 0.7351074309140513, 'reg_lambda': 8.039656444247674e-06, 'reg_alpha': 0.0005527557803550768, 'gamma': 1.441401380939173, 'grow_policy': 'depthwise', 'max_bin': 128, 'max_cat_to_onehot': 1}. Best is trial 12 with value: 0.8847118297041915.\n",
            "[I 2025-09-14 16:21:14,684] Trial 21 finished with value: 0.8839748183620936 and parameters: {'eta': 0.09449912361950302, 'max_depth': 10, 'min_child_weight': 2.680986490025826, 'subsample': 0.6919172843633798, 'colsample_bytree': 0.6821648295735513, 'reg_lambda': 5.379314257633631e-05, 'reg_alpha': 7.11564943300036e-05, 'gamma': 0.5163178962752737, 'grow_policy': 'depthwise', 'max_bin': 64, 'max_cat_to_onehot': 1}. Best is trial 12 with value: 0.8847118297041915.\n",
            "[I 2025-09-14 16:33:32,343] Trial 22 finished with value: 0.884093241391461 and parameters: {'eta': 0.08310145914918873, 'max_depth': 11, 'min_child_weight': 2.0425222958421094, 'subsample': 0.6353329923210019, 'colsample_bytree': 0.6355931299970665, 'reg_lambda': 1.1143225044746594e-06, 'reg_alpha': 0.00015015983827475026, 'gamma': 0.7696001379080359, 'grow_policy': 'depthwise', 'max_bin': 64, 'max_cat_to_onehot': 1}. Best is trial 12 with value: 0.8847118297041915.\n",
            "[I 2025-09-14 16:46:54,160] Trial 23 finished with value: 0.8826573930428182 and parameters: {'eta': 0.05616215768654709, 'max_depth': 11, 'min_child_weight': 2.119873311894508, 'subsample': 0.6309112014017143, 'colsample_bytree': 0.624925502984852, 'reg_lambda': 1.1015201977215461e-06, 'reg_alpha': 1.065663576387477e-05, 'gamma': 1.1269152090155639, 'grow_policy': 'depthwise', 'max_bin': 64, 'max_cat_to_onehot': 1}. Best is trial 12 with value: 0.8847118297041915.\n",
            "[I 2025-09-14 17:01:23,505] Trial 24 finished with value: 0.8807170339359851 and parameters: {'eta': 0.08360969954825875, 'max_depth': 9, 'min_child_weight': 4.7394065000372265, 'subsample': 0.5872229436580263, 'colsample_bytree': 0.5483930247050562, 'reg_lambda': 7.592024357112372e-06, 'reg_alpha': 0.00021785839631151003, 'gamma': 0.8785901276723787, 'grow_policy': 'depthwise', 'max_bin': 64, 'max_cat_to_onehot': 1}. Best is trial 12 with value: 0.8847118297041915.\n",
            "[I 2025-09-14 17:10:26,114] Trial 25 finished with value: 0.8802663175502967 and parameters: {'eta': 0.13988721640813184, 'max_depth': 12, 'min_child_weight': 3.455929005567198, 'subsample': 0.733566486917405, 'colsample_bytree': 0.6459929150371698, 'reg_lambda': 0.2506856508958224, 'reg_alpha': 1.2291033706904755e-05, 'gamma': 3.028446852589185, 'grow_policy': 'depthwise', 'max_bin': 64, 'max_cat_to_onehot': 1}. Best is trial 12 with value: 0.8847118297041915.\n",
            "[I 2025-09-14 17:27:10,328] Trial 26 finished with value: 0.8793832493941567 and parameters: {'eta': 0.023713143169514964, 'max_depth': 11, 'min_child_weight': 1.8063878819064665, 'subsample': 0.5101869762902542, 'colsample_bytree': 0.7883241951538209, 'reg_lambda': 2.0893025083843546e-05, 'reg_alpha': 0.0002766682729679632, 'gamma': 1.9925179756489553, 'grow_policy': 'depthwise', 'max_bin': 64, 'max_cat_to_onehot': 1}. Best is trial 12 with value: 0.8847118297041915.\n",
            "[I 2025-09-14 17:32:00,901] Trial 27 finished with value: 0.8804966864935828 and parameters: {'eta': 0.05806798933169627, 'max_depth': 9, 'min_child_weight': 1.3208768490640816, 'subsample': 0.7862805831564132, 'colsample_bytree': 0.5870987188458973, 'reg_lambda': 0.00043813484913829176, 'reg_alpha': 3.7774657662792595e-06, 'gamma': 5.065575825038412, 'grow_policy': 'depthwise', 'max_bin': 64, 'max_cat_to_onehot': 1}. Best is trial 12 with value: 0.8847118297041915.\n",
            "[I 2025-09-14 17:45:41,984] Trial 28 finished with value: 0.8731181979487485 and parameters: {'eta': 0.1191146107318649, 'max_depth': 7, 'min_child_weight': 10.751117646127675, 'subsample': 0.6123403626662964, 'colsample_bytree': 0.6986481433916367, 'reg_lambda': 4.187456545586913e-06, 'reg_alpha': 2.4196669672505962e-05, 'gamma': 1.74751926687385, 'grow_policy': 'depthwise', 'max_bin': 128, 'max_cat_to_onehot': 1}. Best is trial 12 with value: 0.8847118297041915.\n",
            "[I 2025-09-14 17:50:05,337] Trial 29 finished with value: 0.8764514099166612 and parameters: {'eta': 0.07496779319163716, 'max_depth': 11, 'min_child_weight': 2.3378329315552184, 'subsample': 0.6657973679626907, 'colsample_bytree': 0.5495362370447175, 'reg_lambda': 1.3347423239298386, 'reg_alpha': 0.0007613755245288735, 'gamma': 6.475506007733274, 'grow_policy': 'depthwise', 'max_bin': 64, 'max_cat_to_onehot': 10}. Best is trial 12 with value: 0.8847118297041915.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== HIERARCHICAL — TEST RESULTS ===\n",
            "Confusion Matrix (rows=true, cols=pred):\n",
            "[[238580  16612   1035   1316]\n",
            " [  7036  86228   1454   1743]\n",
            " [  1902   2888  34303    305]\n",
            " [   727    419    168   5284]]\n",
            "Metrics:\n",
            "  accuracy: 0.91099\n",
            "  f1_macro: 0.84656\n",
            "  f1_weighted: 0.91261\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           N     0.9611    0.9264    0.9434    257543\n",
            "           Z     0.8123    0.8939    0.8512     96461\n",
            "           P     0.9281    0.8707    0.8985     39398\n",
            "           F     0.6110    0.8008    0.6932      6598\n",
            "\n",
            "    accuracy                         0.9110    400000\n",
            "   macro avg     0.8281    0.8730    0.8466    400000\n",
            "weighted avg     0.9162    0.9110    0.9126    400000\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# ======================================\n",
        "# Stage A (Binary) — Optuna\n",
        "# ======================================\n",
        "CV_A = StratifiedKFold(n_splits=5, shuffle=True, random_state=123)\n",
        "def split_A():\n",
        "    return CV_A.split(X_trval_A, y_trval_A)\n",
        "\n",
        "def objective_A(trial):\n",
        "    params = {\n",
        "        \"tree_method\": \"hist\",\n",
        "        \"objective\": \"binary:logistic\",\n",
        "        \"eval_metric\": \"aucpr\",\n",
        "        \"eta\": trial.suggest_float(\"eta\", 0.02, 0.20, log=True),\n",
        "        \"max_depth\": trial.suggest_int(\"max_depth\", 4, 12),\n",
        "        \"min_child_weight\": trial.suggest_float(\"min_child_weight\", 1.0, 20.0, log=True),\n",
        "        \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1.0),\n",
        "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.5, 1.0),\n",
        "        \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 1e-6, 50.0, log=True),\n",
        "        \"reg_alpha\":  trial.suggest_float(\"reg_alpha\",  1e-6, 20.0, log=True),\n",
        "        \"gamma\": trial.suggest_float(\"gamma\", 0.0, 10.0),\n",
        "        \"grow_policy\": trial.suggest_categorical(\"grow_policy\", [\"depthwise\", \"lossguide\"]),\n",
        "        \"max_bin\": trial.suggest_categorical(\"max_bin\", [64, 128, 256]),\n",
        "        \"max_cat_to_onehot\": trial.suggest_categorical(\"max_cat_to_onehot\", [1, 2, 10]),\n",
        "        \"sampling_method\": \"uniform\",\n",
        "        \"seed\": 123,\n",
        "    }\n",
        "    f1s = []\n",
        "    for tr_idx, va_idx in split_A():\n",
        "        dtr = xgb.DMatrix(X_trval_A.iloc[tr_idx], label=y_trval_A.iloc[tr_idx],\n",
        "                          weight=sw_A_trval[tr_idx], enable_categorical=True)\n",
        "        dva = xgb.DMatrix(X_trval_A.iloc[va_idx], label=y_trval_A.iloc[va_idx],\n",
        "                          weight=sw_A_trval[va_idx], enable_categorical=True)\n",
        "        booster = xgb.train(params, dtr, num_boost_round=4000, evals=[(dva,\"val\")],\n",
        "                            early_stopping_rounds=150, verbose_eval=False)\n",
        "        proba = booster.predict(dva, iteration_range=(0, booster.best_iteration + 1))\n",
        "        # Max F1 per fold (binary)\n",
        "        grid = np.linspace(0.05, 0.95, 19)\n",
        "        best_f1 = -1.0\n",
        "        for thr in grid:\n",
        "            pred = (proba >= thr).astype(int)\n",
        "            best_f1 = max(best_f1, f1_score(y_trval_A.iloc[va_idx], pred, zero_division=0))\n",
        "        f1s.append(best_f1)\n",
        "    return float(np.mean(f1s))\n",
        "\n",
        "study_A = optuna.create_study(direction=\"maximize\")\n",
        "study_A.optimize(objective_A, n_trials=30, show_progress_bar=False)\n",
        "best_params_A = study_A.best_params.copy()\n",
        "best_params_A.update({\n",
        "    \"tree_method\":\"hist\",\"objective\":\"binary:logistic\",\"eval_metric\":\"aucpr\",\"seed\":123\n",
        "})\n",
        "\n",
        "# OOF to get F1-floor threshold and best n_estimators\n",
        "def get_oof_threshold_and_n_binary(X_ref, y_ref, params, f1_floor=0.90, weights=None):\n",
        "    y_ref = np.asarray(y_ref).astype(int)\n",
        "    oof_proba = np.full(shape=(len(y_ref),), fill_value=np.nan, dtype=float)\n",
        "    best_iters = []\n",
        "    for tr_idx, va_idx in split_A():\n",
        "        dtr = xgb.DMatrix(X_ref.iloc[tr_idx], label=y_ref[tr_idx],\n",
        "                          weight=None if weights is None else np.asarray(weights)[tr_idx],\n",
        "                          enable_categorical=True)\n",
        "        dva = xgb.DMatrix(X_ref.iloc[va_idx], label=y_ref[va_idx],\n",
        "                          weight=None if weights is None else np.asarray(weights)[va_idx],\n",
        "                          enable_categorical=True)\n",
        "        booster = xgb.train(params, dtr, num_boost_round=4000, evals=[(dva,\"val\")],\n",
        "                            early_stopping_rounds=150, verbose_eval=False)\n",
        "        proba = booster.predict(dva, iteration_range=(0, booster.best_iteration + 1))\n",
        "        oof_proba[va_idx] = proba\n",
        "        best_iters.append(int(booster.best_iteration) + 1)\n",
        "    thr = threshold_for_f1(y_ref, oof_proba, f1_floor=f1_floor)\n",
        "    return float(thr), int(np.median(best_iters))\n",
        "\n",
        "thr_A, nA = get_oof_threshold_and_n_binary(X_trval_A, y_trval_A, best_params_A,\n",
        "                                           f1_floor=f1_floor_A, weights=sw_A_trval)\n",
        "\n",
        "# Refit Stage A\n",
        "xgbA = XGBClassifier(\n",
        "    tree_method=\"hist\", objective=\"binary:logistic\",\n",
        "    n_estimators=nA,\n",
        "    learning_rate=best_params_A.get(\"eta\", 0.06),\n",
        "    max_depth=best_params_A.get(\"max_depth\", 8),\n",
        "    min_child_weight=best_params_A.get(\"min_child_weight\", 2.0),\n",
        "    subsample=best_params_A.get(\"subsample\", 0.9),\n",
        "    colsample_bytree=best_params_A.get(\"colsample_bytree\", 0.8),\n",
        "    reg_lambda=best_params_A.get(\"reg_lambda\", 1.0),\n",
        "    reg_alpha=best_params_A.get(\"reg_alpha\", 0.0),\n",
        "    gamma=best_params_A.get(\"gamma\", 0.0),\n",
        "    grow_policy=best_params_A.get(\"grow_policy\", \"depthwise\"),\n",
        "    max_bin=best_params_A.get(\"max_bin\", 256),\n",
        "    max_cat_to_onehot=best_params_A.get(\"max_cat_to_onehot\", 1),\n",
        "    sampling_method=\"uniform\",\n",
        "    random_state=123, n_jobs=-1, enable_categorical=True\n",
        ")\n",
        "xgbA.fit(X_trval_A, y_trval_A, sample_weight=sw_A_trval)\n",
        "\n",
        "# ======================================\n",
        "# Stage B (Multi: Z/P/F) — Optuna\n",
        "# ======================================\n",
        "CV_B = StratifiedKFold(n_splits=5, shuffle=True, random_state=123)\n",
        "def split_B():\n",
        "    return CV_B.split(X_trval_B, y_trval_B)\n",
        "\n",
        "def objective_B(trial):\n",
        "    params = {\n",
        "        \"tree_method\": \"hist\",\n",
        "        \"objective\": \"multi:softprob\",\n",
        "        \"num_class\": 3,\n",
        "        \"eval_metric\": [\"mlogloss\", \"merror\"],\n",
        "        \"eta\": trial.suggest_float(\"eta\", 0.02, 0.20, log=True),\n",
        "        \"max_depth\": trial.suggest_int(\"max_depth\", 4, 12),\n",
        "        \"min_child_weight\": trial.suggest_float(\"min_child_weight\", 1.0, 20.0, log=True),\n",
        "        \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1.0),\n",
        "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.5, 1.0),\n",
        "        \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 1e-6, 50.0, log=True),\n",
        "        \"reg_alpha\":  trial.suggest_float(\"reg_alpha\",  1e-6, 20.0, log=True),\n",
        "        \"gamma\": trial.suggest_float(\"gamma\", 0.0, 10.0),\n",
        "        \"grow_policy\": trial.suggest_categorical(\"grow_policy\", [\"depthwise\", \"lossguide\"]),\n",
        "        \"max_bin\": trial.suggest_categorical(\"max_bin\", [64, 128, 256]),\n",
        "        \"max_cat_to_onehot\": trial.suggest_categorical(\"max_cat_to_onehot\", [1, 2, 10]),\n",
        "        \"sampling_method\": \"uniform\",\n",
        "        \"seed\": 123,\n",
        "    }\n",
        "    f1s = []\n",
        "    for tr_idx, va_idx in split_B():\n",
        "        # Build train/val splits\n",
        "        X_tr = X_trval_B.iloc[tr_idx]\n",
        "        y_tr = y_trval_B.iloc[tr_idx].to_numpy()\n",
        "        X_va = X_trval_B.iloc[va_idx]\n",
        "        y_va = y_trval_B.iloc[va_idx].to_numpy()\n",
        "\n",
        "        # --- Oversample class F on TRAIN ONLY ---\n",
        "        X_tr_os, y_tr_os = oversample_class_to_ratio(\n",
        "            X_tr, y_tr, target_class=F_CLASS_IDX_B, target_ratio=F_TARGET_RATIO_B, random_state=123\n",
        "        )\n",
        "\n",
        "        # DMatrix with NO weights (oversampling already shifts priors)\n",
        "        dtr = xgb.DMatrix(X_tr_os, label=y_tr_os, enable_categorical=True)\n",
        "        dva = xgb.DMatrix(X_va,     label=y_va,     enable_categorical=True)\n",
        "\n",
        "        booster = xgb.train(\n",
        "            params, dtr, num_boost_round=4000, evals=[(dva, \"val\")],\n",
        "            early_stopping_rounds=150, verbose_eval=False\n",
        "        )\n",
        "        proba = booster.predict(dva, iteration_range=(0, booster.best_iteration + 1))\n",
        "        y_pred = proba.argmax(axis=1)\n",
        "        f1s.append(f1_score(y_va, y_pred, average=\"macro\", zero_division=0))\n",
        "    return float(np.mean(f1s))\n",
        "\n",
        "study_B = optuna.create_study(direction=\"maximize\")\n",
        "study_B.optimize(objective_B, n_trials=30, show_progress_bar=False)\n",
        "best_params_B = study_B.best_params.copy()\n",
        "best_params_B.update({\n",
        "    \"tree_method\":\"hist\",\"objective\":\"multi:softprob\",\n",
        "    \"num_class\":3,\"eval_metric\":[\"mlogloss\",\"merror\"],\"seed\":123\n",
        "})\n",
        "\n",
        "def get_oof_thresholds_and_n_multi(X_ref, y_ref, params, f1_floor=0.80, weights=None):\n",
        "    y_ref = np.asarray(y_ref).astype(int)\n",
        "    K = int(len(np.unique(y_ref)))\n",
        "    oof_proba = np.full(shape=(len(y_ref), K), fill_value=np.nan, dtype=float)\n",
        "    best_iters = []\n",
        "    for tr_idx, va_idx in split_B():\n",
        "        X_tr = X_ref.iloc[tr_idx]\n",
        "        y_tr = y_ref[tr_idx]\n",
        "        X_va = X_ref.iloc[va_idx]\n",
        "        y_va = y_ref[va_idx]\n",
        "\n",
        "        # --- Oversample F on TRAIN ONLY ---\n",
        "        X_tr_os, y_tr_os = oversample_class_to_ratio(\n",
        "            X_tr, y_tr, target_class=F_CLASS_IDX_B, target_ratio=F_TARGET_RATIO_B, random_state=123\n",
        "        )\n",
        "\n",
        "        dtr = xgb.DMatrix(X_tr_os, label=y_tr_os, enable_categorical=True)\n",
        "        dva = xgb.DMatrix(X_va,     label=y_va,     enable_categorical=True)\n",
        "\n",
        "        booster = xgb.train(\n",
        "            params, dtr, num_boost_round=4000, evals=[(dva, \"val\")],\n",
        "            early_stopping_rounds=150, verbose_eval=False\n",
        "        )\n",
        "        proba = booster.predict(dva, iteration_range=(0, booster.best_iteration + 1))\n",
        "        oof_proba[va_idx, :] = proba\n",
        "        best_iters.append(int(booster.best_iteration) + 1)\n",
        "\n",
        "    thr_vec = ovr_thresholds_for_f1(y_ref, oof_proba, f1_floor=f1_floor)\n",
        "    return thr_vec, int(np.median(best_iters))\n",
        "\n",
        "# ======================================\n",
        "# Compute Stage B thresholds / n_estimators (CHANGE THE CALL to avoid passing weights)\n",
        "# ======================================\n",
        "thr_vec_B, nB = get_oof_thresholds_and_n_multi(\n",
        "    X_trval_B, y_trval_B, best_params_B, f1_floor=f1_floor_B, weights=None  # weights unused with oversampling\n",
        ")\n",
        "\n",
        "\n",
        "# ======================================\n",
        "# Refit Stage B (REPLACE this block to oversample on the full train+val pool)\n",
        "# ======================================\n",
        "# Oversample 'F' on the full Stage-B training pool before final fit\n",
        "X_trval_B_os, y_trval_B_os = oversample_class_to_ratio(\n",
        "    X_trval_B, y_trval_B.to_numpy(), target_class=F_CLASS_IDX_B,\n",
        "    target_ratio=F_TARGET_RATIO_B, random_state=123\n",
        ")\n",
        "\n",
        "xgbB = XGBClassifier(\n",
        "    tree_method=\"hist\", objective=\"multi:softprob\", num_class=3,\n",
        "    n_estimators=nB,\n",
        "    learning_rate=best_params_B.get(\"eta\", 0.06),\n",
        "    max_depth=best_params_B.get(\"max_depth\", 8),\n",
        "    min_child_weight=best_params_B.get(\"min_child_weight\", 2.0),\n",
        "    subsample=best_params_B.get(\"subsample\", 0.9),\n",
        "    colsample_bytree=best_params_B.get(\"colsample_bytree\", 0.8),\n",
        "    reg_lambda=best_params_B.get(\"reg_lambda\", best_params_B.get(\"lambda\", 1.0)),\n",
        "    reg_alpha=best_params_B.get(\"reg_alpha\", 0.0),\n",
        "    gamma=best_params_B.get(\"gamma\", 0.0),\n",
        "    grow_policy=best_params_B.get(\"grow_policy\", \"depthwise\"),\n",
        "    max_bin=best_params_B.get(\"max_bin\", 256),\n",
        "    max_cat_to_onehot=best_params_B.get(\"max_cat_to_onehot\", 1),\n",
        "    sampling_method=\"uniform\",\n",
        "    random_state=123, n_jobs=-1, enable_categorical=True\n",
        ")\n",
        "# No sample_weight here — we already oversampled\n",
        "xgbB.fit(X_trval_B_os, y_trval_B_os)\n",
        "\n",
        "# ======================================\n",
        "# Compose FINAL predictions on TEST\n",
        "# ======================================\n",
        "# Stage A decision\n",
        "probaA_test = xgbA.predict_proba(X_test)[:, 1]\n",
        "predA_test  = (probaA_test >= thr_A).astype(int)  # 1 = Denied, 0 = NotDenied\n",
        "\n",
        "# Stage B (for all; use only where predA=1)\n",
        "probaB_test = xgbB.predict_proba(X_test)         # n x 3\n",
        "thrB = np.asarray(thr_vec_B, dtype=float)\n",
        "overB = probaB_test >= thrB\n",
        "any_overB = overB.any(axis=1)\n",
        "scores_overB = np.where(overB, probaB_test, -np.inf)\n",
        "predB_all = scores_overB.argmax(axis=1)\n",
        "predB_all[~any_overB] = probaB_test[~any_overB].argmax(axis=1)\n",
        "\n",
        "# Merge to full labels (codes)\n",
        "pred_full_codes = np.full(shape=(len(X_test),), fill_value=code_N, dtype=int)  # default 'N'\n",
        "map_B_to_full = {i: inv_map_full[DENIED_LABELS[i]] for i in range(3)}\n",
        "denied_idx = np.where(predA_test == 1)[0]\n",
        "pred_full_codes[denied_idx] = np.vectorize(map_B_to_full.get)(predB_all[denied_idx])\n",
        "\n",
        "# Evaluate\n",
        "cmT, metT, repT = evaluate_predictions(y_full_test, pred_full_codes, class_names=class_names_full)\n",
        "print_results(\"HIERARCHICAL — TEST\", cmT, metT, repT)\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved: xgb_claims_multicat_model_2stage_20250914_180851.joblib\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# ======================================\n",
        "# Save artifacts\n",
        "# ======================================\n",
        "feature_cols = X_all.columns.tolist()\n",
        "final_artifacts = {\n",
        "    \"stageA_model\": xgbA,\n",
        "    \"stageA_threshold\": float(thr_A),\n",
        "    \"stageB_model\": xgbB,\n",
        "    \"stageB_thresholds\": [float(x) for x in thr_vec_B],\n",
        "    \"feature_cols\": feature_cols,\n",
        "    \"categorical_cols\": categorical_cols,\n",
        "    \"cat_dtypes\": cat_dtype_map,\n",
        "    \"class_names_full\": class_names_full,\n",
        "    \"class_names_B\": DENIED_LABELS,\n",
        "    \"params_A\": best_params_A,\n",
        "    \"params_B\": best_params_B,\n",
        "}\n",
        "# Format current datetime as YYYYMMDD_HHMMSS\n",
        "current_time = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "filename = f\"xgb_claims_multicat_model_2stage_{current_time}.joblib\"\n",
        "dump(final_artifacts, filename)\n",
        "print(f\"Saved: {filename}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved UNSEEN predictions: xgb_unseen_predictions_hier_2025-09-14_18-09-14.csv\n",
            "Clinic                       Service  AmountCharged CPTCode  ClientID                           Payer   Provider AuthStatus eligStatus MultiFlag  SameDayCli  DaysBetServiceToBilling  proba_deny  proba_Z  proba_P  proba_F  xgb_pred_code xgb_pred MultiFlag_true\n",
            "  VWBY            Take Home Dose MMT          25.40   H0020       132          HealthPartners PMAP MN 1932375110        N/A        N/A         Z           0                        7    0.999974 0.999569 0.000053 0.000378              1        Z              Z\n",
            "  B42D         Individual Counseling          96.00   H0004      2156           Anthem HealthKeepers  1922584366        N/A        N/A         N           0                        3    0.039964 0.010896 0.000730 0.988374              0        N              N\n",
            "    RE E&M New Patient Visit - 99205         144.62   99205      5500             Anthem Medicaid NV  1427042118        N/A        N/A         F           0                        7    0.993303 0.084133 0.006064 0.909803              3        F              F\n",
            "   B29            Take Home Dose MMT          15.50   H0020      8313                 CSTAR Medicaid  1629789961        N/A        N/A         N           0                        2    0.000002 0.100923 0.894826 0.004252              0        N              N\n",
            "   B35    Methadone Maintenance Week         114.00   H0020      7959 Passport Health Plan by Molina  1609086057        N/A        N/A         N           0                        2    0.000773 0.606989 0.124356 0.268655              0        N              N\n",
            "\n",
            "=== Full 4-class report (N / Z / P / F) ===\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           N     0.9608    0.9266    0.9434      6432\n",
            "           Z     0.8071    0.8955    0.8490      2355\n",
            "           P     0.9242    0.8549    0.8882      1027\n",
            "           F     0.6197    0.7796    0.6905       186\n",
            "\n",
            "    accuracy                         0.9092     10000\n",
            "   macro avg     0.8280    0.8642    0.8428     10000\n",
            "weighted avg     0.9145    0.9092    0.9108     10000\n",
            "\n",
            "Saved 4-class confusion matrix: classification_confusion_full_2025-09-14_18-09-14.csv\n",
            "\n",
            "=== Stage A report (N vs Denied) ===\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           N     0.9608    0.9266    0.9434      6432\n",
            "      Denied     0.8757    0.9319    0.9029      3568\n",
            "\n",
            "    accuracy                         0.9285     10000\n",
            "   macro avg     0.9183    0.9293    0.9232     10000\n",
            "weighted avg     0.9304    0.9285    0.9290     10000\n",
            "\n",
            "Saved Stage A confusion matrix: classification_confusion_stageA_2025-09-14_18-09-14.csv\n",
            "\n",
            "=== Stage B report (Z / P / F) on rows passing Stage A ===\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           Z     0.9556    0.9643    0.9599      2187\n",
            "           P     0.9543    0.9024    0.9276       973\n",
            "           F     0.7323    0.8788    0.7989       165\n",
            "\n",
            "    accuracy                         0.9420      3325\n",
            "   macro avg     0.8808    0.9152    0.8955      3325\n",
            "weighted avg     0.9442    0.9420    0.9425      3325\n",
            "\n",
            "Saved Stage B confusion matrix: classification_confusion_stageB_2025-09-14_18-09-14.csv\n"
          ]
        }
      ],
      "source": [
        "# ======================================\n",
        "# PREDICT ON df_unseen (if provided)\n",
        "# ======================================\n",
        "\n",
        "# Preconditions (must be defined earlier in your script)\n",
        "assert 'drop_dt' in globals()\n",
        "assert 'align_to_training_native' in globals()\n",
        "assert 'feature_cols' in globals()\n",
        "assert 'cat_dtype_map' in globals()\n",
        "assert 'final_artifacts' in globals()\n",
        "assert 'DENIED_LABELS' in globals()          # e.g., ['Z','P','F']\n",
        "assert 'class_names_full' in globals()       # e.g., ['N','Z','P','F'] (order matters)\n",
        "assert 'map_B_to_full' in globals()          # maps Stage B idx {0,1,2} -> int code in class_names_full\n",
        "assert 'code_N' in globals()                 # int code for 'N' in class_names_full (typically 0)\n",
        "\n",
        "if 'df_unseen' in globals():\n",
        "    # --- Align to training schema ---\n",
        "    df_unseen_sc = df_unseen.copy()\n",
        "    df_unseen_sc = df_unseen_sc.drop(columns=drop_dt, errors='ignore')\n",
        "    X_unseen = align_to_training_native(df_unseen_sc, feature_cols, cat_dtype_map)\n",
        "\n",
        "    # ---------- Stage A (N vs Denied) ----------\n",
        "    pA = final_artifacts[\"stageA_model\"].predict_proba(X_unseen)[:, 1]\n",
        "    denied_pred = (pA >= final_artifacts[\"stageA_threshold\"]).astype(int)  # numpy array\n",
        "\n",
        "    # ---------- Stage B (Z/P/F) ----------\n",
        "    pB = final_artifacts[\"stageB_model\"].predict_proba(X_unseen)            # shape: (n, 3)\n",
        "    thrB = np.asarray(final_artifacts[\"stageB_thresholds\"], dtype=float)    # shape: (3,)\n",
        "    overB = pB >= thrB                                                       # broadcast → (n, 3)\n",
        "    any_overB = overB.any(axis=1)\n",
        "    scores_overB = np.where(overB, pB, -np.inf)\n",
        "    predB_all = scores_overB.argmax(axis=1)\n",
        "    # Fallback to argmax if none of the class probs clear thresholds\n",
        "    predB_all[~any_overB] = pB[~any_overB].argmax(axis=1)\n",
        "\n",
        "    # ---------- Compose final 4-class predictions ----------\n",
        "    pred_full_code = np.full(len(X_unseen), fill_value=code_N, dtype=int)\n",
        "    idx_denied = np.where(denied_pred == 1)[0]\n",
        "    # Map Stage B indices (0/1/2) to full-space integer codes for class_names_full\n",
        "    pred_full_code[idx_denied] = np.vectorize(map_B_to_full.get)(predB_all[idx_denied])\n",
        "\n",
        "    labels_full = [class_names_full[i] for i in pred_full_code]             # predicted string labels\n",
        "\n",
        "    # ---------- Package outputs ----------\n",
        "    results_unseen = df_unseen.copy()\n",
        "    results_unseen[\"proba_deny\"] = pA\n",
        "    for i, lbl in enumerate(DENIED_LABELS):\n",
        "        results_unseen[f\"proba_{lbl}\"] = pB[:, i]\n",
        "    results_unseen[\"xgb_pred_code\"] = pred_full_code\n",
        "    results_unseen[\"xgb_pred\"] = labels_full\n",
        "\n",
        "    # Attach ground truth if present\n",
        "    if \"MultiFlag\" in df_unseen.columns:\n",
        "        results_unseen[\"MultiFlag_true\"] = df_unseen[\"MultiFlag\"]\n",
        "\n",
        "    ts = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
        "    out_pred_fn = f\"xgb_unseen_predictions_hier_{ts}.csv\"\n",
        "    results_unseen.to_csv(out_pred_fn, index=False)\n",
        "    print(\"Saved UNSEEN predictions:\", out_pred_fn)\n",
        "\n",
        "    # Optional: safe preview without hiding real errors\n",
        "    try:\n",
        "        print(results_unseen.head().to_string(index=False))\n",
        "    except (UnicodeEncodeError, BrokenPipeError) as e:\n",
        "        print(f\"(Skipping preview due to console issue: {e.__class__.__name__})\")\n",
        "\n",
        "    # ---------- Classification reports ----------\n",
        "    if \"MultiFlag\" in df_unseen.columns:\n",
        "        mf = df_unseen[\"MultiFlag\"]\n",
        "\n",
        "        # Normalize y_true to string labels matching class_names_full\n",
        "        if pd.api.types.is_numeric_dtype(mf):\n",
        "            # Treat numbers as codes into class_names_full\n",
        "            y_true_full = []\n",
        "            for x in mf.values:\n",
        "                if pd.isna(x):\n",
        "                    y_true_full.append(None)\n",
        "                else:\n",
        "                    xi = int(x)\n",
        "                    y_true_full.append(class_names_full[xi] if 0 <= xi < len(class_names_full) else None)\n",
        "        else:\n",
        "            y_true_full = mf.astype(str).tolist()\n",
        "\n",
        "        # Convert to NumPy arrays (FIX for list + NumPy index mismatch)\n",
        "        y_true_full = np.asarray(y_true_full, dtype=object)\n",
        "        y_pred_full = np.asarray(labels_full)\n",
        "\n",
        "        # ---- Full 4-class report (N/Z/P/F) ----\n",
        "        valid_mask_full = np.isin(y_true_full, class_names_full)\n",
        "        y_true_full_f = y_true_full[valid_mask_full]\n",
        "        y_pred_full_f = y_pred_full[valid_mask_full]\n",
        "\n",
        "        print(\"\\n=== Full 4-class report (N / Z / P / F) ===\")\n",
        "        print(classification_report(\n",
        "            y_true_full_f, y_pred_full_f,\n",
        "            labels=class_names_full, digits=4, zero_division=0\n",
        "        ))\n",
        "\n",
        "        cm_full = confusion_matrix(y_true_full_f, y_pred_full_f, labels=class_names_full)\n",
        "        cm_full_df = pd.DataFrame(\n",
        "            cm_full,\n",
        "            index=[f\"true_{l}\" for l in class_names_full],\n",
        "            columns=[f\"pred_{l}\" for l in class_names_full]\n",
        "        )\n",
        "        cm_full_fn = f\"classification_confusion_full_{ts}.csv\"\n",
        "        cm_full_df.to_csv(cm_full_fn, index=True)\n",
        "        print(\"Saved 4-class confusion matrix:\", cm_full_fn)\n",
        "\n",
        "        # ---- Stage A (binary: N vs Denied) ----\n",
        "        y_true_A = np.where(y_true_full == 'N', 0, np.where(np.isin(y_true_full, DENIED_LABELS), 1, -1))\n",
        "        valid_mask_A = (y_true_A >= 0)\n",
        "        y_true_A_f = y_true_A[valid_mask_A]\n",
        "        denied_pred_f = denied_pred[valid_mask_A]\n",
        "\n",
        "        print(\"\\n=== Stage A report (N vs Denied) ===\")\n",
        "        print(classification_report(\n",
        "            y_true_A_f, denied_pred_f,\n",
        "            labels=[0, 1], target_names=['N', 'Denied'],\n",
        "            digits=4, zero_division=0\n",
        "        ))\n",
        "\n",
        "        cm_A = confusion_matrix(y_true_A_f, denied_pred_f, labels=[0, 1])\n",
        "        cm_A_df = pd.DataFrame(cm_A, index=['true_N', 'true_Denied'], columns=['pred_N', 'pred_Denied'])\n",
        "        cm_A_fn = f\"classification_confusion_stageA_{ts}.csv\"\n",
        "        cm_A_df.to_csv(cm_A_fn, index=True)\n",
        "        print(\"Saved Stage A confusion matrix:\", cm_A_fn)\n",
        "\n",
        "        # ---- Stage B (Z/P/F) on rows where truth is Denied AND Stage A predicted Denied ----\n",
        "        idx_stageB_eval = np.where(np.isin(y_true_full, DENIED_LABELS) & (denied_pred == 1))[0]\n",
        "        if idx_stageB_eval.size > 0:\n",
        "            y_true_B = y_true_full[idx_stageB_eval]\n",
        "            y_pred_B = y_pred_full[idx_stageB_eval]\n",
        "\n",
        "            print(\"\\n=== Stage B report (Z / P / F) on rows passing Stage A ===\")\n",
        "            print(classification_report(\n",
        "                y_true_B, y_pred_B,\n",
        "                labels=DENIED_LABELS, digits=4, zero_division=0\n",
        "            ))\n",
        "\n",
        "            cm_B = confusion_matrix(y_true_B, y_pred_B, labels=DENIED_LABELS)\n",
        "            cm_B_df = pd.DataFrame(\n",
        "                cm_B,\n",
        "                index=[f\"true_{l}\" for l in DENIED_LABELS],\n",
        "                columns=[f\"pred_{l}\" for l in DENIED_LABELS]\n",
        "            )\n",
        "            cm_B_fn = f\"classification_confusion_stageB_{ts}.csv\"\n",
        "            cm_B_df.to_csv(cm_B_fn, index=True)\n",
        "            print(\"Saved Stage B confusion matrix:\", cm_B_fn)\n",
        "        else:\n",
        "            print(\"\\n[Stage B] No rows where truth is Denied and Stage A predicted Denied; skipping Stage B report.\")\n",
        "    else:\n",
        "        print(\"\\nGround truth ('MultiFlag') not found in df_unseen; classification reports skipped.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\nSQL = \\'\\'\\'\\nSELECT *\\nFROM pats.vw_ClaimsDenialPrediction\\nWHERE ClaimBillDate > \\'2025-08-11\\' AND LIATPCLIid IS NULL\\n\\'\\'\\'\\nCNXN_STR = (\\n    \"Driver={ODBC Driver 17 for SQL Server};\"\\n    \"Server=bhgazuresql01.database.windows.net;\"\\n    \"Authentication=ActiveDirectoryPassword;\"\\n    \"Database=xxxxxxx;\"\\n    \"UID=xxxxxxx;\"\\n    \"PWD=xxxxxxx;\"\\n)\\n\\nprint(\"Connecting to Azure SQL...\")\\ncnxn = pyodbc.connect(CNXN_STR)\\ndf_raw = pd.read_sql(SQL, cnxn)\\ncnxn.close()\\nprint(f\"Loaded {len(df_raw):,} rows.\")\\n\\n# Keep original columns; drop raw datetime cols as in training\\ndf_out = df_raw.copy()\\ndf_feat = df_raw.drop(columns=drop_dt, errors=\\'ignore\\')\\n\\n# Align + predict\\nX_future = align_to_training_native(df_feat, final_artifacts[\"feature_cols\"], final_artifacts[\"cat_dtypes\"])\\n\\n# Stage A\\npA = final_artifacts[\"stageA_model\"].predict_proba(X_future)[:, 1]\\ndenied_pred = (pA >= final_artifacts[\"stageA_threshold\"]).astype(int)\\n\\n# Stage B\\npB = final_artifacts[\"stageB_model\"].predict_proba(X_future)\\nthrB = np.asarray(final_artifacts[\"stageB_thresholds\"], dtype=float)\\noverB = pB >= thrB\\nany_overB = overB.any(axis=1)\\nscores_overB = np.where(overB, pB, -np.inf)\\npredB_all = scores_overB.argmax(axis=1)\\npredB_all[~any_overB] = pB[~any_overB].argmax(axis=1)\\n\\n# Compose\\npred_full = np.full(shape=(len(X_future),), fill_value=code_N, dtype=int)\\nidx_d = np.where(denied_pred == 1)[0]\\npred_full[idx_d] = np.vectorize(map_B_to_full.get)(predB_all[idx_d])\\nlabels_full = [class_names_full[i] for i in pred_full]\\n\\n# Attach predictions\\nfor i, lbl in enumerate(DENIED_LABELS):\\n    df_out[f\"proba_{lbl}\"] = pB[:, i]\\ndf_out[\"proba_deny\"] = pA\\ndf_out[\"xgb_pred_code\"] = pred_full\\ndf_out[\"xgb_pred\"] = labels_full\\n\\nprint(df_out.head())\\nprint(\"Predicted class distribution:\", df_out[\"xgb_pred\"].value_counts().to_dict())\\n\\nts = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\\nfile_name = f\"xgb_prediction_hier_{ts}.csv\"\\ndf_out.to_csv(file_name, index=False)\\nprint(f\"Saved: {file_name}\")\\n'"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "# ======================================\n",
        "# Azure: Predict future claims (Hierarchical)\n",
        "# ======================================\n",
        "# Uncomment/configure to run\n",
        "\"\"\"\n",
        "SQL = '''\n",
        "SELECT *\n",
        "FROM pats.vw_ClaimsDenialPrediction\n",
        "WHERE ClaimBillDate > '2025-08-11' AND LIATPCLIid IS NULL\n",
        "'''\n",
        "CNXN_STR = (\n",
        "    \"Driver={ODBC Driver 17 for SQL Server};\"\n",
        "    \"Server=bhgazuresql01.database.windows.net;\"\n",
        "    \"Authentication=ActiveDirectoryPassword;\"\n",
        "    \"Database=xxxxxxx;\"\n",
        "    \"UID=xxxxxxx;\"\n",
        "    \"PWD=xxxxxxx;\"\n",
        ")\n",
        "\n",
        "print(\"Connecting to Azure SQL...\")\n",
        "cnxn = pyodbc.connect(CNXN_STR)\n",
        "df_raw = pd.read_sql(SQL, cnxn)\n",
        "cnxn.close()\n",
        "print(f\"Loaded {len(df_raw):,} rows.\")\n",
        "\n",
        "# Keep original columns; drop raw datetime cols as in training\n",
        "df_out = df_raw.copy()\n",
        "df_feat = df_raw.drop(columns=drop_dt, errors='ignore')\n",
        "\n",
        "# Align + predict\n",
        "X_future = align_to_training_native(df_feat, final_artifacts[\"feature_cols\"], final_artifacts[\"cat_dtypes\"])\n",
        "\n",
        "# Stage A\n",
        "pA = final_artifacts[\"stageA_model\"].predict_proba(X_future)[:, 1]\n",
        "denied_pred = (pA >= final_artifacts[\"stageA_threshold\"]).astype(int)\n",
        "\n",
        "# Stage B\n",
        "pB = final_artifacts[\"stageB_model\"].predict_proba(X_future)\n",
        "thrB = np.asarray(final_artifacts[\"stageB_thresholds\"], dtype=float)\n",
        "overB = pB >= thrB\n",
        "any_overB = overB.any(axis=1)\n",
        "scores_overB = np.where(overB, pB, -np.inf)\n",
        "predB_all = scores_overB.argmax(axis=1)\n",
        "predB_all[~any_overB] = pB[~any_overB].argmax(axis=1)\n",
        "\n",
        "# Compose\n",
        "pred_full = np.full(shape=(len(X_future),), fill_value=code_N, dtype=int)\n",
        "idx_d = np.where(denied_pred == 1)[0]\n",
        "pred_full[idx_d] = np.vectorize(map_B_to_full.get)(predB_all[idx_d])\n",
        "labels_full = [class_names_full[i] for i in pred_full]\n",
        "\n",
        "# Attach predictions\n",
        "for i, lbl in enumerate(DENIED_LABELS):\n",
        "    df_out[f\"proba_{lbl}\"] = pB[:, i]\n",
        "df_out[\"proba_deny\"] = pA\n",
        "df_out[\"xgb_pred_code\"] = pred_full\n",
        "df_out[\"xgb_pred\"] = labels_full\n",
        "\n",
        "print(df_out.head())\n",
        "print(\"Predicted class distribution:\", df_out[\"xgb_pred\"].value_counts().to_dict())\n",
        "\n",
        "ts = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
        "file_name = f\"xgb_prediction_hier_{ts}.csv\"\n",
        "df_out.to_csv(file_name, index=False)\n",
        "print(f\"Saved: {file_name}\")\n",
        "\"\"\"\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "py313",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
